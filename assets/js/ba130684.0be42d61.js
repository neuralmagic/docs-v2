"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[7471],{4783:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var r=s(5893),t=s(1151);const a={sidebar_position:2,title:"Convert LLMs from HF"},l="I have a HF model, how do I get it in DeepSparse?",i={id:"guides/hf-llm-to-deepsparse",title:"Convert LLMs from HF",description:"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse.",source:"@site/docs/guides/hf-llm-to-deepsparse.mdx",sourceDirName:"guides",slug:"/guides/hf-llm-to-deepsparse",permalink:"/docs-v2/guides/hf-llm-to-deepsparse",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/guides/hf-llm-to-deepsparse.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Convert LLMs from HF"},sidebar:"autogenerated_docs",previous:{title:"Why is Sparsity Important for LLMs?",permalink:"/docs-v2/guides/why-weight-sparsity"},next:{title:"Compress LLMs with SparseGPT",permalink:"/docs-v2/guides/one-shot-llms-with-sparseml"}},o={},d=[{value:"Export",id:"export",level:2},{value:"Note on system requirements",id:"note-on-system-requirements",level:3},{value:"Benchmark",id:"benchmark",level:2},{value:"Inference",id:"inference",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"i-have-a-hf-model-how-do-i-get-it-in-deepsparse",children:"I have a HF model, how do I get it in DeepSparse?"}),"\n",(0,r.jsx)(n.p,{children:"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse."}),"\n",(0,r.jsx)(n.h2,{id:"export",children:"Export"}),"\n",(0,r.jsx)(n.p,{children:"Install SparseML with support for HF Transformers:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install sparseml-nightly[transformers]\n"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.h3,{id:"note-on-system-requirements",children:"Note on system requirements"}),"\n",(0,r.jsxs)(n.p,{children:["Due to inefficiencies in PyTorch ONNX export, a lot of system memory is required to export the models for inference. There are ",(0,r.jsx)(n.a,{href:"https://github.com/pytorch/pytorch/commit/b4a49124c8165a374a3ef49e14807ac05b3fc030",children:"improvements coming in torch>=2.2 so use the latest version possible"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Download the weights of the model you want from HF Hub:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0 --local-dir original_model --local-dir-use-symlinks False\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Export the model to ONNX (this produces the model in a new folder called ",(0,r.jsx)(n.code,{children:"deployment/"}),")"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sparseml.export --task text-generation original_model/\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Optional: You can upload that ",(0,r.jsx)(n.code,{children:"deployment/"})," folder back up to HF Hub for later use. See this ",(0,r.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds/tree/main",children:"TinyLlama"})," as an example."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"huggingface-cli upload --repo-type model username/your-model-id original_model/deployment/\n"})}),"\n",(0,r.jsx)(n.h2,{id:"benchmark",children:"Benchmark"}),"\n",(0,r.jsxs)(n.p,{children:["Benchmarking was performed on an AWS m7i.4xlarge instance using ",(0,r.jsx)(n.code,{children:"deepsparse-nightly[llm]==1.7.0.20240103"})," with FP32 dense and sparse Llama models finetuned on GSM8k - full details of those models can be found in the ",(0,r.jsx)(n.a,{href:"https://neuralmagic.com/blog/fast-llama-2-on-cpus-with-sparse-fine-tuning-and-deepsparse/",children:"release blog"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["These benchmarks used ",(0,r.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?architectures=llama2&datasets=gsm8k&ungrouped=true",children:"models from SparseZoo"}),", as seen from the prepended ",(0,r.jsx)(n.code,{children:"zoo:"}),", but you can also use exported models hosted on Hugging Face by prepending with ",(0,r.jsx)(n.code,{children:"hf:"})," such as ",(0,r.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds",children:(0,r.jsx)(n.code,{children:"hf:neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds"})}),"."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sparsity"}),(0,r.jsx)(n.th,{children:"Decode tokens/s"}),(0,r.jsx)(n.th,{children:"Decode Speedup"}),(0,r.jsx)(n.th,{children:"Prefill tokens/s (128 token input)"}),(0,r.jsx)(n.th,{children:"Prefill Speedup"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0%"}),(0,r.jsx)(n.td,{children:"3.63"}),(0,r.jsx)(n.td,{children:"1.00"}),(0,r.jsx)(n.td,{children:"66.06"}),(0,r.jsx)(n.td,{children:"1.00"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"50%"}),(0,r.jsx)(n.td,{children:"6.44"}),(0,r.jsx)(n.td,{children:"1.77"}),(0,r.jsx)(n.td,{children:"91.53"}),(0,r.jsx)(n.td,{children:"1.39"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"60%"}),(0,r.jsx)(n.td,{children:"7.79"}),(0,r.jsx)(n.td,{children:"2.14"}),(0,r.jsx)(n.td,{children:"101.71"}),(0,r.jsx)(n.td,{children:"1.54"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"70%"}),(0,r.jsx)(n.td,{children:"9.82"}),(0,r.jsx)(n.td,{children:"2.70"}),(0,r.jsx)(n.td,{children:"115.87"}),(0,r.jsx)(n.td,{children:"1.75"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"80%"}),(0,r.jsx)(n.td,{children:"13.17"}),(0,r.jsx)(n.td,{children:"3.62"}),(0,r.jsx)(n.td,{children:"140.62"}),(0,r.jsx)(n.td,{children:"2.13"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Benchmarking commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export NM_BENCHMARK_KV_TOKENS=1\n\n# Decode benchmarking: Time to generate a token aka generated token/s\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-base\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned50\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned70\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned80\n\n# Prefill benchmarking: Time to process 128 tokens aka prefill token/s\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-base\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned50\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned70\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned80\n"})}),"\n",(0,r.jsx)(n.h2,{id:"inference",children:"Inference"}),"\n",(0,r.jsx)(n.p,{children:"Install DeepSparse LLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install deepsparse-nightly[llm]\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then simply load your model using the ",(0,r.jsx)(n.code,{children:"deepsparse.TextGeneration"})," class"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import TextGeneration\n\nmodel = TextGeneration(model_path="zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60_quantized")\nprompt = "James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?"\nprint(model(prompt).generations[0].text)\n"""\nFirst find the total number of meters James runs in one sprint: 60 meters/sprint * 3 sprints = <<60*3=180>>180 meters\nThen multiply that number by the number of sprints per week to find the total number of meters he runs each week: 180 meters/sprint * 3 sprints/week = <<180*3=540>>540 meters/week\n540\n"""\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},1151:(e,n,s)=>{s.d(n,{Z:()=>i,a:()=>l});var r=s(7294);const t={},a=r.createContext(t);function l(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);