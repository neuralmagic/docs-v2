"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[7301],{3901:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});var r=a(5893),t=a(1151);const s={tags:["Neural Magic","DeepSparse","SparseZoo","SparseML","AI Deployment","Model Optimization"],keywords:["Neural Network Deployment","Sparsity","Performance Optimization","AI Inference","Model Benchmarking"],description:"A comprehensive guide for deploying neural networks with Neural Magic\u2019s DeepSparse, SparseZoo, and SparseML, focusing on text generation and object detection.",sidebar_label:"Deploy",sidebar_position:2},l="Deployment",i={id:"get-started/deploy",title:"Deployment",description:"A comprehensive guide for deploying neural networks with Neural Magic\u2019s DeepSparse, SparseZoo, and SparseML, focusing on text generation and object detection.",source:"@site/docs/get-started/deploy.mdx",sourceDirName:"get-started",slug:"/get-started/deploy",permalink:"/docs-v2/get-started/deploy",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/get-started/deploy.mdx",tags:[{label:"Neural Magic",permalink:"/docs-v2/tags/neural-magic"},{label:"DeepSparse",permalink:"/docs-v2/tags/deep-sparse"},{label:"SparseZoo",permalink:"/docs-v2/tags/sparse-zoo"},{label:"SparseML",permalink:"/docs-v2/tags/sparse-ml"},{label:"AI Deployment",permalink:"/docs-v2/tags/ai-deployment"},{label:"Model Optimization",permalink:"/docs-v2/tags/model-optimization"}],version:"current",sidebarPosition:2,frontMatter:{tags:["Neural Magic","DeepSparse","SparseZoo","SparseML","AI Deployment","Model Optimization"],keywords:["Neural Network Deployment","Sparsity","Performance Optimization","AI Inference","Model Benchmarking"],description:"A comprehensive guide for deploying neural networks with Neural Magic\u2019s DeepSparse, SparseZoo, and SparseML, focusing on text generation and object detection.",sidebar_label:"Deploy",sidebar_position:2},sidebar:"autogenerated_docs",previous:{title:"SparseZoo",permalink:"/docs-v2/get-started/install/sparsezoo"},next:{title:"Sparsify",permalink:"/docs-v2/get-started/optimize"}},o={},d=[{value:"Causal Language Modeling - LLMs",id:"causal-language-modeling---llms",level:2},{value:"Pipeline",id:"pipeline",level:3},{value:"Server",id:"server",level:3},{value:"Performance",id:"performance",level:3},{value:"Benchmarking",id:"benchmarking",level:4},{value:"Accuracy",id:"accuracy",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components},{Details:a,TabItem:s,Tabs:l}=n;return a||h("Details",!0),s||h("TabItem",!0),l||h("Tabs",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"deployment",children:"Deployment"}),"\n",(0,r.jsx)(n.p,{children:"This guide provides detailed instructions for deploying neural networks using Neural Magic's suite, including DeepSparse, SparseZoo, and SparseML.\nYou'll learn about the key processes such as benchmarking, pipeline creation, and server setup, focusing on text generation models."}),"\n",(0,r.jsx)(n.h2,{id:"causal-language-modeling---llms",children:"Causal Language Modeling - LLMs"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Ensure you've installed the required packages and dependencies for Generative AI as outlined in the ",(0,r.jsx)(n.a,{href:"./install#generative-ai",children:"Install Guide"}),"."]})}),"\n",(0,r.jsx)(n.p,{children:"This section covers deploying large language models (LLMs) with DeepSparse including benchmarking, pipeline creation, and server setup utilizing a pre-sparsified model from the SparseZoo."}),"\n",(0,r.jsx)(n.p,{children:"The examples below use a sparse, quantized Llama 2 7b chat model to demonstrate the deployment process.\nThe model is referenced by the following SparseZoo stub for use in the Neural Magic suite:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized\n"})}),"\n",(0,r.jsxs)(n.p,{children:["For other models that work with these examples, browse through the ",(0,r.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?modelSet=generative_ai",children:"Generative AI models in the SparseZoo"})," to find one that fits your needs."]}),"\n",(0,r.jsx)(n.h3,{id:"pipeline",children:"Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"DeepSparse pipelines for LLMs match the Hugging Face Transformers Python API, allowing for familiarity and easy integration with existing codebases.\nTo create a text generation pipeline for the example model and generate text, utilize the following code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import TextGeneration\n\npipeline = TextGeneration(\n    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized"\n)\nresult = pipeline("Large language models are")\nprint(result)\n'})}),"\n",(0,r.jsx)(n.p,{children:"The output will show the generated text based on the input prompt."}),"\n",(0,r.jsx)(n.h3,{id:"server",children:"Server"}),"\n",(0,r.jsx)(n.p,{children:"The DeepSparse Server wraps a pipeline in a REST API, allowing for easy deployment and inference.\nFor generative LLMS, the server supports the OpenAI inference standards, allowing for familiarity and easy integration with existing codebases."}),"\n",(0,r.jsx)(n.p,{children:"To create a DeepSparse TextGeneration server that will run on port 5543 (default) with the OpenAI specs, utilize the following code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.server \\\n  --integration openai \\\n  "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Now with the server running, you can send an HTTP request that conforms to the OpenAI spec to generate text.\nBelow are examples for using ",(0,r.jsx)(n.code,{children:"curl"})," and ",(0,r.jsx)(n.code,{children:"python"})," to send a request to the server:"]}),"\n",(0,r.jsxs)(l,{children:[(0,r.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import openai\n\nopenai.api_key = "EMPTY"\nopenai.api_base = "http://localhost:5543/v1"\n\ncompletion = openai.ChatCompletion.create(\n    messages="Neural Magic is",\n    stream=True,\n    max_tokens=30,\n    model="zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",\n)\nfor token in completion:\n    print(token)\n'})})}),(0,r.jsx)(s,{value:"bash",label:"Bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:5543/v1/chat/completions \\\n-H "Content-Type: application/json" \\\n-d \'{\n    "model": "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",\n    "messages": "Neural Magic is",\n    "stream": true\n}\'\n'})})})]}),"\n",(0,r.jsx)(n.p,{children:"The output will show the generated text from the server based on the input prompt."}),"\n",(0,r.jsx)(n.h3,{id:"performance",children:"Performance"}),"\n",(0,r.jsx)(n.p,{children:"Various performance metrics for inference performance and accuracy can be evaluated using DeepSparse.\nThe following sections demonstrate how to benchmark the model and evaluate its accuracy.\nThe sections below are optional and can be skipped."}),"\n",(0,r.jsx)(n.h4,{id:"benchmarking",children:"Benchmarking"}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Baseline"}),(0,r.jsx)(n.p,{children:"To compare the performance of the sparsified model, we utilize a baseline, unoptimized version of the model from the SparseZoo.\nThe stub for the corresponding model is:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-llama2_pretrain-base\n"})}),(0,r.jsx)(n.p,{children:"The following command utilizes the baseline stub and DeepSparse to establish the unoptimized model's performance:"}),(0,r.jsxs)(l,{children:[(0,r.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse.benchmark import benchmark_model\n\nresult = benchmark_model("zoo:llama2-7b-llama2_pretrain-base")\nprint(result)\n'})})}),(0,r.jsx)(s,{value:"bash",label:"Bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.benchmark "zoo:llama2-7b-llama2_pretrain-base"\n'})})})]}),(0,r.jsxs)(n.p,{children:["On an 8-core AMD CPU, the baseline model achieves a throughput around ",(0,r.jsx)(n.code,{children:"2.7"})," tokens per second."]})]}),"\n",(0,r.jsx)(n.p,{children:"The following command utilizes the sparsified model and DeepSparse to establish the optimized model's performance:"}),"\n",(0,r.jsxs)(l,{children:[(0,r.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import benchmark\n\nresult = benchmark("zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized")\nprint(result)\n'})})}),(0,r.jsx)(s,{value:"bash",label:"Bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.benchmark "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized"\n'})})})]}),"\n",(0,r.jsxs)(n.p,{children:["On an 8-core AMD CPU, the sparsified model achieves a throughput around ",(0,r.jsx)(n.code,{children:"13.1"})," tokens per second, which is ",(0,r.jsx)(n.code,{children:"4.9"})," times faster than the baseline model!"]}),"\n",(0,r.jsx)(n.h4,{id:"accuracy",children:"Accuracy"}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Baseline"}),(0,r.jsx)(n.p,{children:"To compare the performance of the sparsified model, we utilize a baseline, unoptimized version of the model from the SparseZoo.\nThe stub for the corresponding model is:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-ultrachat200k_llama2_pretrain-base\n"})}),(0,r.jsx)(n.p,{children:"The following command utilizes the baseline stub and DeepSparse to establish the unoptimized model's accuracy:"}),(0,r.jsxs)(l,{children:[(0,r.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import evaluate\n\nresult = evaluate(\n    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base",\n    integration="lm-eval-harness"\n)\nprint(result)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,r.jsx)(s,{value:"bash",label:"Bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.evaluate \\\n    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base" \\\n    --integration "lm-eval-harness"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]}),(0,r.jsxs)(n.p,{children:["As seen in the output, the baseline model achieves a throughput of ",(0,r.jsx)(n.code,{children:"x"})," tokens per second on a 4-core Intel CPU."]})]}),"\n",(0,r.jsxs)(n.p,{children:["The following command utilizes the sparsified model and DeepSparse to establish the optimized model's accuracy within the ",(0,r.jsx)(n.code,{children:"lm-eval-harness"}),":"]}),"\n",(0,r.jsxs)(l,{children:[(0,r.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import evaluate\n\nresult = evaluate(\n    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",\n    integration="lm-eval-harness"\n)\nprint(result)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,r.jsx)(s,{value:"bash",label:"Bash",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.evaluate \\\n    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" \\\n    --integration "lm-eval-harness"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]}),"\n",(0,r.jsxs)(n.p,{children:["As seen in the output, the sparsified model achieves a score of ",(0,r.jsx)(n.code,{children:"y"}),", which is within ",(0,r.jsx)(n.code,{children:"z"})," of the baseline model!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"For more information on the example tasks in this guide, other supported tasks, and custom integrations dive into the following resources:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../generative-ai",children:"Generative AI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../computer-vision",children:"Computer Vision"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../nlp",children:"Natural Language Processing"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../custom-integrations",children:"Custom Integrations"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}function h(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},1151:(e,n,a)=>{a.d(n,{Z:()=>i,a:()=>l});var r=a(7294);const t={},s=r.createContext(t);function l(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);