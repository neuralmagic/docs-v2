"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[7111],{5077:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var t=a(5893),i=a(1151);const s={tags:["Sparsification","Large Language Models","Generative AI"],keywords:["SparseML","SparseZoo","DeepSparse","LLM Sparsification"],description:"This guide provides an overview of applying sparsification techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",sidebar_label:"Sparsify",sidebar_position:3},r="Sparsify",o={id:"get-started/sparsify",title:"Sparsify",description:"This guide provides an overview of applying sparsification techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",source:"@site/docs/get-started/sparsify.mdx",sourceDirName:"get-started",slug:"/get-started/sparsify",permalink:"/docs-v2/get-started/sparsify",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/get-started/sparsify.mdx",tags:[{label:"Sparsification",permalink:"/docs-v2/tags/sparsification"},{label:"Large Language Models",permalink:"/docs-v2/tags/large-language-models"},{label:"Generative AI",permalink:"/docs-v2/tags/generative-ai"}],version:"current",sidebarPosition:3,frontMatter:{tags:["Sparsification","Large Language Models","Generative AI"],keywords:["SparseML","SparseZoo","DeepSparse","LLM Sparsification"],description:"This guide provides an overview of applying sparsification techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",sidebar_label:"Sparsify",sidebar_position:3},sidebar:"autogenerated_docs",previous:{title:"Deploy",permalink:"/docs-v2/get-started/deploy"},next:{title:"Sparse Finetuning",permalink:"/docs-v2/get-started/finetune"}},l={},d=[{value:"Causal Language Modeling - LLMs",id:"causal-language-modeling---llms",level:2},{value:"Data Preparation",id:"data-preparation",level:3},{value:"One Shot",id:"one-shot",level:3},{value:"One Shot with Finetuning",id:"one-shot-with-finetuning",level:3},{value:"Inference",id:"inference",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components},{Details:a,TabItem:s,Tabs:r}=n;return a||c("Details",!0),s||c("TabItem",!0),r||c("Tabs",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"sparsify",children:"Sparsify"}),"\n",(0,t.jsx)(n.p,{children:"This section provides a technical overview of getting started with sparsifying neural networks to improve inference performance, specifically LLMs, using Neural Magic's SparseML for model optimization, SparseZoo for pre-trained models, and DeepSparse for accelerated inference."}),"\n",(0,t.jsx)(n.h2,{id:"causal-language-modeling---llms",children:"Causal Language Modeling - LLMs"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Ensure you have the necessary packages installed for Generative AI as outlined in the ",(0,t.jsx)(n.a,{href:"./install#generative-ai",children:"Install Guide"}),"."]})}),"\n",(0,t.jsx)(n.p,{children:"Sparsifying LLMs involves the use of SparseML to apply pruning and quantization techniques, enhancing model efficiency without significant losses in performance and leveraging DeepSparse for efficient inference on CPUs."}),"\n",(0,t.jsx)(n.p,{children:"The examples below demonstrate the process of sparsifying a Mistral 7b model from Hugging Face's model hub:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"mistralai/Mistral-7B-Instruct-v0.2\n"})}),"\n",(0,t.jsxs)(n.p,{children:["For additional models, explore the ",(0,t.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?modelSet=generative_ai",children:"Generative AI models in the SparseZoo"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"data-preparation",children:"Data Preparation"}),"\n",(0,t.jsx)(n.p,{children:"SparseML requires a dataset to be used for both one shot methods and fine-tuning.\nThe dataset is used in one shot as a calibration set and in finetuning as a training set."}),"\n",(0,t.jsx)(n.p,{children:"For the examples below, we use the Open Platypus dataset, which is available in the Hugging Face dataset hub:"}),"\n",(0,t.jsxs)(r,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\n\ndataset = load_dataset("garage-bAInd/Open-Platypus")\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'--dataset "garage-bAInd/Open-Platypus"\n'})})})]}),"\n",(0,t.jsx)(n.p,{children:"Replace the dataset with your own dataset or use the provided example.\nThe simplest way is to use either a Hugging Face dataset or a list of tokenized examples as shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import SparseAutoTokenizer\n\ntokenizer = SparseAutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\nsamples = [\n    "The platypus is a mammal.",\n    "The platypus lays eggs.",\n    "The platypus is venomous."\n]\ndataset = [tokenizer(sample, return_tensors="pt") for sample in samples]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For comprehensive data preparation guidelines including formats like CSV and JSONL, refer to our ",(0,t.jsx)(n.a,{href:"../generative-ai/causal-language-modeling/data",children:"detailed datasets guide"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"one-shot",children:"One Shot"}),"\n",(0,t.jsx)(n.p,{children:"Applying pruning and quantization to a SparseGPT model in one shot is a simple and quick process utilizing SparseML resulting in smaller, faster, and more efficient models.\nTo do so, we construct the model, dataset, and utilize SparseML to apply Modifiers to the model which apply SOTA pruning and quantization techniques:"}),"\n",(0,t.jsxs)(r,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import (\n    SparseAutoModelForCausalLM, SparseAutoTokenizer, load_dataset, apply\n)\nfrom sparseml.modifiers import SparseGPTModifier\n\nmodel = SparseAutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ntokenizer = SparseAutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ndataset = load_dataset("garage-bAInd/Open-Platypus")\n\nmodifier = SparseGPTModifier(\n    sparsity=0.5,\n    quantize=True,\n)\napply(\n    model=model,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    modifiers=modifier,\n    output_dir="./one-shot-example",\n)\n\n# <output>\n# TODO\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'echo "\nmodifiers:\n    SparseGPTModifier\n        sparsity: 0.5\n        quantize: True\n" > one-shot.yaml\n\nsparseml.transformers.apply \\\n    --model "mistralai/Mistral-7B-Instruct-v0.2" \\\n    --tokenizer "mistralai/Mistral-7B-Instruct-v0.2" \\\n    --dataset "garage-bAInd/Open-Platypus" \\\n    --recipe "./one-shot.yaml" \\\n    --output-dir "./one-shot-example"\n\n# <output>\n# TODO\n# </output>\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["As seen in the output summary, the model is pruned and quantized, resulting in a smaller and faster model.\nIf you want to improve the accuracy of the model, use the ",(0,t.jsx)(n.a,{href:"#one-shot-with-finetuning",children:"fine-tuning"})," method.\nOtherwise, the model is ready for efficient inference and dive into the ",(0,t.jsx)(n.a,{href:"#inference",children:"inference"})," section for next steps."]}),"\n",(0,t.jsx)(n.h3,{id:"one-shot-with-finetuning",children:"One Shot with Finetuning"}),"\n",(0,t.jsx)(n.p,{children:"After pruning the model, finetuning can help in retaining performance post-sparsification.\nTo enable this, we use the same model, dataset, and SparseML to apply Modifiers in stages to the model.\nSpecifically, we first apply pruning in one shot, then finetuning the model on the same dataset, and finally apply quantization in one shot.\nThe process is as follows:"}),"\n",(0,t.jsxs)(r,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import (\n    SparseAutoModelForCausalLM, SparseAutoTokenizer, load_dataset, apply\n)\nfrom sparseml.modifiers import SparseGPTModifier, TrainingModifier\nfrom sparseml.core import RecipeStage\n\nmodel = SparseAutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ntokenizer = SparseAutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ndataset = load_dataset("garage-bAInd/Open-Platypus")\n\nmodifiers = [\n    RecipeStage(\n        modifiers=[\n            SparseGPTModifier(sparsity=0.5)\n        ]\n    ),\n    RecipeStage(\n        modifiers=[\n            TrainingModifier(epochs=2),\n            ConstantPruningModifier(),\n        ]\n    ),\n    RecipeStage(\n        modifiers=[\n            SparseGPTModifier(quantize=True)\n        ]\n    ),\n]\napply(\n    model=model,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    modifiers=modifiers,\n    output_dir="./one-shot-finetune-example",\n)\n\n# <output>\n# TODO\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'echo "\npruning_stage:\n    modifiers:\n        - !SparseGPTModifier\n            sparsity: 0.5\nfinetune_stage:\n    modifiers:\n        TrainingModifier:\n            epochs: 2\n        ConstantPruningModifier:\n            start: 0\nquantize_stage:\n    modifiers:\n        - !SparseGPTModifier\n            quantize: True\n" > one-shot-finetune.yaml\n\nsparseml.transformers.apply \\\n    --model "mistralai/Mistral-7B-Instruct-v0.2" \\\n    --tokenizer "mistralai/Mistral-7B-Instruct-v0.2" \\\n    --dataset "garage-bAInd/Open-Platypus" \\\n    --recipe "./one-shot-finetune.yaml" \\\n    --output-dir "./one-shot-finetune-example"\n\n# <output>\n# TODO\n# </output>\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["As seen in the output, the model is pruned and quantized while retaining better perplexity.\nThe model is now ready for efficient inference and can be used in the ",(0,t.jsx)(n.a,{href:"#inference",children:"inference"})," section for next steps."]}),"\n",(0,t.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Be sure to replace the sample models provided below with the models you have sparsified.\nA sample, sparsified Mistral model from the SparseZoo is used by default."})}),"\n",(0,t.jsxs)(a,{children:[(0,t.jsx)("summary",{children:"Evaluating Accuracy"}),(0,t.jsxs)(n.p,{children:["It is important to evaluate the accuracy of the model to ensure it meets the desired performance requirements.\nTo do so, we can use the following code to evaluate the model across a number of benchmarks available in the ",(0,t.jsx)(n.code,{children:"lm-eval-harness"}),":"]}),(0,t.jsxs)(r,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import evaluate\n\nevaluate(\n    "zoo:mistral-7b-open_platypus_orca_mistral_pretrain-pruned50_quantized",\n    integration="lm-eval-harness"\n)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.evaluate \\\n    "zoo:mistral-7b-open_platypus_orca_mistral_pretrain-pruned50_quantized" \\\n    --integration "lm-eval-harness"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]})]}),"\n",(0,t.jsx)(n.p,{children:"After sparsifying the model, it can be used for inference in PyTorch with the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer\n\nmodel_path = "zoo:mistral-7b-open_platypus_orca_mistral_pretrain-pruned50_quantized"\nmodel = SparseAutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = SparseAutoTokenizer.from_pretrained(model_path)\ninputs = tokenizer(["Neural Magic is"], return_tensors="pt")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(outputs)\n\n# <output>\n# TODO\n# </output>\n'})}),"\n",(0,t.jsx)(n.p,{children:"The above code; however, does not leverage the sparsity within the model for efficient inference.\nTo do so, we first export the model to ONNX and inject KV caching logic so that it is ready for efficient inference on CPUs with DeepSparse:"}),"\n",(0,t.jsxs)(r,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import export\n\nexport(\n    "zoo:mistral-7b-open_platypus_orca_mistral_pretrain-pruned50_quantized",\n    task="text-generation",\n    sequence_length=1024,\n    target_path="./exported"\n)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.export \\\n    "zoo:mistral-7b-open_platypus_orca_mistral_pretrain-pruned50_quantized" \\\n    --task "text-generation" \\\n    --sequence_length 1024 \\\n    --target_path "./exported"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["The exported model located at ",(0,t.jsx)(n.code,{children:"./exported"})," can now be used for efficient inference with DeepSparse.\nTo do so, sub in the exported model within the ",(0,t.jsx)(n.a,{href:"../getting-started/deploy",children:"Getting Started - Deploy"})," guide for your desired deployment method."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"Explore more on sparsification and efficient deployment of AI models across various domains with Neural Magic's suite:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../generative-ai",children:"Generative AI"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../computer-vision",children:"Computer Vision"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../nlp",children:"Natural Language Processing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../custom-integrations",children:"Custom Integrations"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}function c(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},1151:(e,n,a)=>{a.d(n,{Z:()=>o,a:()=>r});var t=a(7294);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);