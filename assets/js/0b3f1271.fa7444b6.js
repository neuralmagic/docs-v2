"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[849],{3238:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var t=a(5893),r=a(1151);const s={tags:["Neural Magic","DeepSparse","SparseZoo","SparseML","Model Finetuning","AI Training"],keywords:["Neural Network Finetuning","Sparse Training","Model Adaptation","AI Enhancement","Model Customization"],description:"This guide provides an overview of applying sparse finetuning techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",sidebar_label:"Sparse Finetuning",sidebar_position:4},i="Sparse Finetuning",o={id:"get-started/finetune",title:"Sparse Finetuning",description:"This guide provides an overview of applying sparse finetuning techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",source:"@site/docs/get-started/finetune.mdx",sourceDirName:"get-started",slug:"/get-started/finetune",permalink:"/docs-v2/get-started/finetune",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/get-started/finetune.mdx",tags:[{label:"Neural Magic",permalink:"/docs-v2/tags/neural-magic"},{label:"DeepSparse",permalink:"/docs-v2/tags/deep-sparse"},{label:"SparseZoo",permalink:"/docs-v2/tags/sparse-zoo"},{label:"SparseML",permalink:"/docs-v2/tags/sparse-ml"},{label:"Model Finetuning",permalink:"/docs-v2/tags/model-finetuning"},{label:"AI Training",permalink:"/docs-v2/tags/ai-training"}],version:"current",sidebarPosition:4,frontMatter:{tags:["Neural Magic","DeepSparse","SparseZoo","SparseML","Model Finetuning","AI Training"],keywords:["Neural Network Finetuning","Sparse Training","Model Adaptation","AI Enhancement","Model Customization"],description:"This guide provides an overview of applying sparse finetuning techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.",sidebar_label:"Sparse Finetuning",sidebar_position:4},sidebar:"autogenerated_docs",previous:{title:"Sparsify",permalink:"/docs-v2/get-started/sparsify"},next:{title:"Serving LLMs",permalink:"/docs-v2/serving-llms"}},l={},d=[{value:"Causual Language Modeling - LLMs",id:"causual-language-modeling---llms",level:2},{value:"Data Preparation",id:"data-preparation",level:3},{value:"Training",id:"training",level:3},{value:"Inference",id:"inference",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...e.components},{Details:a,TabItem:s,Tabs:i}=n;return a||c("Details",!0),s||c("TabItem",!0),i||c("Tabs",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"sparse-finetuning",children:"Sparse Finetuning"}),"\n",(0,t.jsx)(n.p,{children:"This section provides a technical overview of getting started with fine-tuning pre-sparsified neural networks to improve performance on specific tasks or datasets, specifically LLMs, using Neural Magic's SparseML for model adaptation, SparseZoo for pre-trained models, and DeepSparse for accelerated inference."}),"\n",(0,t.jsx)(n.h2,{id:"causual-language-modeling---llms",children:"Causual Language Modeling - LLMs"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Ensure you have the necessary packages installed for Generative AI as outlined in the ",(0,t.jsx)(n.a,{href:"./install#generative-ai",children:"Install Guide"}),"."]})}),"\n",(0,t.jsx)(n.p,{children:"Sparse finetuning LLMs involves the use of SparseML to adapt a pre-sparsified model to a specific task or dataset, enhancing its performance for the targeted use case and leveraging DeepSparse for efficient inference on CPUs."}),"\n",(0,t.jsx)(n.p,{children:"The examples below demostrate the process of sparse finetuning a pre-sparsified Llama 7b model from the SparseZoo:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50\n"})}),"\n",(0,t.jsx)(n.p,{children:"Utilizing the following recipe available with the model, which maintains the sparsity of the model while adapting it to the targeted task or dataset and then utilizes SparseGPT to quantize the model after finetuning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"recipe_type=transfer_quantize\n"})}),"\n",(0,t.jsxs)(n.p,{children:["For additional models, explore the ",(0,t.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?modelSet=generative_ai",children:"Generative AI models in the SparseZoo"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"data-preparation",children:"Data Preparation"}),"\n",(0,t.jsx)(n.p,{children:"SparseML requires a dataset to be used for both one shot methods and fine-tuning.\nThe dataset is used as the training set during finetuning."}),"\n",(0,t.jsx)(n.p,{children:"For the examples below, we use the Helpful instructions dataset from the Hugging Face H4 team, which is available in the Hugging Face dataset hub:"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\n\ndataset = load_dataset("HuggingFaceH4/helpful_instructions")\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'--dataset "HuggingFaceH4/helpful_instructions"\n'})})})]}),"\n",(0,t.jsx)(n.p,{children:"Replace the dataset with your own dataset or use the provided example.\nThe simplest way is to use either a Hugging Face dataset or a list of examples and a tokenizer as shown below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import SparseAutoTokenizer\n\ntokenizer = SparseAutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ndataset = [\n    {"prompt": "Make a list of 10 ways to help students improve their study skills. Output:", "completion": "1. Make a schedule for studying and stick to it. 2. Study in the same place every time. 3. Set goals for yourself. 4. Take breaks when you need them. 5. Don\'t cram before an exam. 6. Get enough sleep. 7. Eat healthy food. 8. Exercise regularly. 9. Find a study partner. 10. Reward yourself after completing a task."},\n    {"prompt": "Given a list of numbers, find out if they are prime or not. you can answer "prime" or "not prime". Input: List: 1, 4, 6, 8, 9 Output:", "completion": "not prime"},\n    {"prompt": "Should i take this class or not? The professor of this class is not good at all. He doesn\'t teach well and he is always late for class.", "completion": "No"}\n]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For comprehensive data preparation guidelines including formats like CSV and JSONL, refer to our ",(0,t.jsx)(n.a,{href:"../generative-ai/causal-language-modeling/data",children:"detailed datasets guide"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"training",children:"Training"}),"\n",(0,t.jsx)(n.p,{children:"Training the sparse model further on the targeted dataset utilizing SparseML and Recipes is simple and straightforward.\nIn doing this, the performant, sparse architecture is adapted to the specific task or dataset, enhancing its performance for the targeted use case.\nTo start the training process, utilize the following command:"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import (\n    SparseAutoModelForCausalLM, SparseAutoTokenizer, load_dataset, apply\n)\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50"\n)\ntokenizer = SparseAutoTokenizer.from_pretrained(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50"\n)\ndataset = load_dataset("HuggingFaceH4/helpful_instructions")\n\napply(\n    model=model,\n    tokenizer=tokenizer,\n    data=dataset,\n    recipe="zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize",\n    recipe_args={"epochs": 1},\n)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.transformers.text_generation.apply \\\n    --model "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50" \\\n    --data "HuggingFaceH4/helpful_instructions" \\\n    --recipe "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize" \\\n    --recipe-args \'{"epochs": 1}\'\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["Once the training process is complete, the model will be saved to the ",(0,t.jsx)(n.code,{children:"./output"})," directory by default."]}),"\n",(0,t.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,t.jsxs)(n.admonition,{type:"note",children:[(0,t.jsx)(n.p,{children:"Be sure to replace the sample models provided below with the model you finetuned.\nA sample, sparse-quantized model from the SparseZoo is used by default with the following stub:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized\n"})})]}),"\n",(0,t.jsxs)(a,{children:[(0,t.jsx)("summary",{children:"Evaluating Accuracy"}),(0,t.jsxs)(n.p,{children:["It is important to evaluate the accuracy of the model to ensure it meets the desired performance requirements.\nTo do so, we can use the following code to evaluate the model across a number of benchmarks available in the ",(0,t.jsx)(n.code,{children:"lm-eval-harness"}),":"]}),(0,t.jsxs)(i,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import evaluate\n\nevaluate(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",\n    integration="lm-eval-harness"\n)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.evaluate \\\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \\\n    --integration "lm-eval-harness"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]})]}),"\n",(0,t.jsx)(n.p,{children:"After sparse finetuning the model, it can be used for inference in PyTorch with the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer\n\nmodel_path = "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized"\nmodel = SparseAutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = SparseAutoTokenizer.from_pretrained(model_path)\ninputs = tokenizer(["Neural Magic is"], return_tensors="pt")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(outputs)\n\n# <output>\n# TODO\n# </output>\n'})}),"\n",(0,t.jsx)(n.p,{children:"The above code; however, does not leverage the sparsity within the model for efficient inference.\nTo do so, we first export the model to ONNX and inject KV caching logic so that it is ready for efficient inference on CPUs with DeepSparse:"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)(s,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import export\n\nexport(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",\n    task="text-generation",\n    sequence_length=1024,\n    target_path="./exported"\n)\n\n# <output>\n# TODO: add output\n# </output>\n'})})}),(0,t.jsx)(s,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.export \\\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \\\n    --task "text-generation" \\\n    --sequence_length 1024 \\\n    --target_path "./exported"\n\n# <output>\n# TODO: add output\n# </output>\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["The exported model located at ",(0,t.jsx)(n.code,{children:"./exported"})," can now be used for efficient inference with DeepSparse.\nTo do so, sub in the exported model within the ",(0,t.jsx)(n.a,{href:"../getting-started/deploy",children:"Getting Started - Deploy"})," guide for your desired deployment method."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"For more information on the example tasks in this guide, other supported tasks, and custom integrations, explore the following resources:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../generative-ai",children:"Generative AI"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../computer-vision",children:"Computer Vision"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../nlp",children:"Natural Language Processing"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../custom-integrations",children:"Custom Integrations"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}function c(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},1151:(e,n,a)=>{a.d(n,{Z:()=>o,a:()=>i});var t=a(7294);const r={},s=t.createContext(r);function i(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);