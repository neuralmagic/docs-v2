"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[2721],{4950:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>l});var o=r(4848),s=r(8453);const a={sidebar_position:1},t="FAQs",i={id:"details/faqs",title:"FAQs",description:"General Product FAQs",source:"@site/docs/details/faqs.mdx",sourceDirName:"details",slug:"/details/faqs",permalink:"/docs-v2/next/details/faqs",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/details/faqs.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"autogenerated_docs",previous:{title:"Details",permalink:"/docs-v2/next/details/"},next:{title:"Glossary",permalink:"/docs-v2/next/details/glossary"}},c={},l=[{value:"General Product FAQs",id:"general-product-faqs",level:2},{value:"Benchmarking FAQs",id:"benchmarking-faqs",level:2},{value:"Infrastructure FAQs",id:"infrastructure-faqs",level:2},{value:"Model Compression FAQs",id:"model-compression-faqs",level:2},{value:"Runtime FAQs",id:"runtime-faqs",level:2}];function d(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",hr:"hr",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"faqs",children:"FAQs"}),"\n",(0,o.jsx)(n.h2,{id:"general-product-faqs",children:"General Product FAQs"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What is Neural Magic?"})}),"\n",(0,o.jsxs)(n.p,{children:["Neural Magic was founded by a team of award-winning MIT computer scientists and is funded by Amdocs, Andreessen Horowitz, Comcast Ventures, NEA, Pillar\nVC, Ridgeline Partners, Verizon Ventures, and VMWare. The Neural Magic Platform includes several components, including ",(0,o.jsx)(n.a,{href:"/products/deepsparse",children:"DeepSparse"}),", ",(0,o.jsx)(n.a,{href:"/products/sparseml",children:"SparseML"}),", and ",(0,o.jsx)(n.a,{href:"/products/sparsezoo",children:"SparseZoo"}),".\nDeepSparse is an inference runtime offering GPU-class performance on CPUs and tooling to\nintegrate ML into your application. ",(0,o.jsx)(n.a,{href:"/products/sparseml",children:"SparseML"})," and ",(0,o.jsx)(n.a,{href:"/products/sparsezoo",children:"SparseZoo,"})," are open-source tooling and a model repository\ncombination that enables you to create an inference-optimized sparse-model for deployment with DeepSparse."]}),"\n",(0,o.jsx)(n.p,{children:"Together, these components remove the tradeoff between performance and the simplicity and scalability of software-delivered deployments."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What is DeepSparse?"})}),"\n",(0,o.jsx)(n.p,{children:"DeepSparse, created by Neural Magic, is an inference runtime for deep learning models. It delivers state of art, GPU-class performance on commodity CPUs\nas well as tooling for integrating a model into an application and monitoring models in production."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Why Neural Magic?"})}),"\n",(0,o.jsxs)(n.p,{children:["Learn more about Neural Magic and DeepSparse (formerly known as the Neural Magic Inference Engine).\n",(0,o.jsx)(n.a,{href:"https://youtu.be/zJy_8uPZd0o",children:"Watch the Why Neural Magic video"})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"How does Neural Magic make it work?"})}),"\n",(0,o.jsxs)(n.p,{children:["This is an older webinar (50m) where we went through the process of optimizing and deploying a model; we\u2019ve enhanced our software since\nthe recording went out but this will give you some background: ",(0,o.jsx)(n.a,{href:"https://youtu.be/UhmmHTsfrzI",children:"Watch the How Does it Work video"})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Does Neural Magic support training of learning models on CPUs?"})}),"\n",(0,o.jsx)(n.p,{children:"Neural Magic does not support training of deep learning models at this time. We do see value in providing a consistent CPU environment\nfor our end users to train and infer on for their deep learning needs, and we have added this to our engineering backlog."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you run on AMD hardware?"})}),"\n",(0,o.jsxs)(n.p,{children:["DeepSparse is validated to work on x86 Intel (Haswell generation and later) and AMD CPUs running Linux, with\nsupport for AVX2, AVX-512, and VNNI instruction sets. Specific support details for some algorithms over different microarchitectures\n",(0,o.jsx)(n.a,{href:"/user-guides/deepsparse-engine/hardware-support",children:"is available"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"We are open to opportunities to expand our support footprint for different CPU-based processor architectures, based on\nmarket adoption and deep learning use cases."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you run on ARM architecture?"})}),"\n",(0,o.jsxs)(n.p,{children:["We are continuing to work on ARM support, primarily focused on LLMs and transformer models for server-grade systems like AWS Graviton and Ampere Currently, we have limited alpha support for CNN models on embedded systems, particularly those with dot product instructions (ARMv8.2+). ARM and MacOS has beta support. Feel free to ",(0,o.jsx)(n.em,{children:"pip install deepsparse-nightyly"})," if you would like to try it out. We would like to hear your use cases and keep you in the\nloop! ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/contact/",children:"Contact us to continue the conversation"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"To what use cases is the Neural Magic Platform best suited?"})}),"\n",(0,o.jsx)(n.p,{children:"We focus on the models and use cases related to computer vision and NLP due to cost sensitivity and both real-time and throughput constraints.\nThe belief now is GPUs are required for deployment."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What types of models does Neural Magic support?"})}),"\n",(0,o.jsxs)(n.p,{children:["Today, we offer support for CNN-based computer vision models, specifically classification and object detection model types.\nNLP models like BERT are also available. We are continuously adding models to ",(0,o.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com",children:"the SparseZoo."}),"\nAdditionally, we are investigating model architectures beyond computer vision."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Is dynamic shape supported?"})}),"\n",(0,o.jsxs)(n.p,{children:["Dynamic shape is currently not supported; be sure to use models with fixed inputs and compile the model for a particular batch size.\nDynamic shape and dynamic batch sizes are on the Neural Magic roadmap; ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/subscribe/",children:"subscribe for updates."})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Can multiple model inferences be executed?"})}),"\n",(0,o.jsxs)(n.p,{children:["Model inferences are executed as a single stream by default; concurrent execution ",(0,o.jsx)(n.a,{href:"/user-guides/deepsparse-engine/scheduler",children:"can be enabled depending\non the engine execution strategy."})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"benchmarking-faqs",children:"Benchmarking FAQs"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you have benchmarks to compare and contrast?"})}),"\n",(0,o.jsxs)(n.p,{children:["Yes. Check out our ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/blog/neural-magic-demo/",children:"benchmark demo video"})," or\n",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/contact/",children:"contact us"})," to discuss your particular performance requirements.\nIf you\u2019d rather observe performance for yourself, ",(0,o.jsx)(n.a,{href:"https://github.com/neuralmagic",children:"head over to the Neural Magic GitHub repo"}),"\nto check out our tools and generate your own benchmarks in your environment."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you publish ML Perf inference benchmarks?"})}),"\n",(0,o.jsxs)(n.p,{children:["Checkout ZDNet's coverage of our ",(0,o.jsx)(n.a,{href:"https://www.zdnet.com/article/neural-magics-sparsity-nvidias-hopper-and-alibabas-network-among-firsts-in-latest-mlperf-ai-benchmarks/",children:"results at ML Perf"}),"!"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"infrastructure-faqs",children:"Infrastructure FAQs"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Which instruction sets are supported and do we have to enable certain settings?"})}),"\n",(0,o.jsx)(n.p,{children:"AVX2, AVX-512, and VNNI. DeepSparse will automatically utilize the most effective available\ninstructions for the task. Depending on your goals and hardware priorities, optimal performance can be found.\nNeural Magic is happy to discuss your use cases and offer recommendations."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Are you suitable for edge deployments (i.e., in-store devices, cameras)?"})}),"\n",(0,o.jsx)(n.p,{children:"Yes, absolutely. We can run anywhere you have a CPU with x86 instructions, including on bare metal, in the cloud,\non-prem, or at the edge. Additionally, our model optimization tools are able to reduce the footprint of models\nacross all architectures. We only guarantee performance in DeepSparse."}),"\n",(0,o.jsxs)(n.p,{children:["We\u2019d love to hear from users highly interested in ML performance. If you want to chat about your use cases\nor how others are leveraging the Neural Magic Platform, ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/contact/",children:"please contact us."}),"\nOr simply head over to the ",(0,o.jsx)(n.a,{href:"https://github.com/neuralmagic",children:"Neural Magic GitHub repo"})," and check out our tools."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you have available solutions or applications on the Microsoft/Azure platform?"})}),"\n",(0,o.jsx)(n.p,{children:"We deploy extremely easily. We are completely infrastructure-agnostic. As long as it has the \u201cright\u201d CPUs\n(e.g., AVX2 or AVX-512) we can run on any cloud platform, including Azure!"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Can the inference engine run on Kubernetes? How do you containerize and take advantage of underlying infrastructure?"})}),"\n",(0,o.jsx)(n.p,{children:"DeepSparse becomes a component of your model-serving solution. As a result, it can\nsimply plug into an existing CI/CD deployment pipeline. How you deploy, where you deploy, and what you deploy on\nbecomes abstracted to DeepSparse so you can tailor your experiences. For example, you can run the\nDeepSparse on a CPU VM environment, deployed via a Docker file, and managed through a Kubernetes environment."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"model-compression-faqs",children:"Model Compression FAQs"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Can you comment on how you do pruning and its effects on accuracy?"})}),"\n",(0,o.jsx)(n.p,{children:"Neural networks are extremely over-parameterized, allowing most weights to be iteratively removed from the network\nwithout effect on accuracy. Eventually, though, pruning will begin affecting the overall capacity of the network,\nthe degree of which varies based on the use case. However, this is something entirely under the control of the data scientist\nto choose whether to recover fully or to prune more for even better performance."}),"\n",(0,o.jsxs)(n.p,{children:["For example, Neural Magic has been successful in removing 95% of ResNet-50 weights with no loss in accuracy.\nFor more background on techniques that have informed our methodologies, check out this paper co-written by\nNeural Magic, ",(0,o.jsx)(n.em,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2004.14340",children:"WoodFisher: Efficient Second-Order Approximation for Neural Network Compression."})})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"When does sparsification actually happen?"})}),"\n",(0,o.jsxs)(n.p,{children:["In a scenario in which you want to sparsify and then run your own model with DeepSparse, you would first\nsparsify your model to achieve the desired level of performance and accuracy using Neural Magic\u2019s ",(0,o.jsx)(n.a,{href:"/products/sparseml",children:"SparseML"})," tooling."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What does the sparsification process look like?"})}),"\n",(0,o.jsxs)(n.p,{children:["Neural Magic\u2019s Sparsify and SparseML tooling, at its core, uses well-established state-of-the-art research principles such as\n",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/blog/pruning-gmp/",children:"Gradual Magnitude Pruning"})," (GMP) to sparsify models. This is an iterative process\nin which groups of important weights are pruned away and then the network is allowed to recover. To significantly simplify the process,\nwe offer tools and guidance for you to achieve the best performance possible. To peruse research papers contributed by Neural Magic\nstaff, ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/resources/technical-papers/",children:"check them out."})," Or head over to the ",(0,o.jsx)(n.a,{href:"https://github.com/neuralmagic",children:"Neural Magic GitHub repo"}),"\nto get started!"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"How does sparsification work in relation to TensorFlow?"})}),"\n",(0,o.jsx)(n.p,{children:"Today, we are able to sparsify models trained in popular deep learning libraries like TensorFlow. Our unique approach works with the\noutput supplied by the model library and provides layer sparsification techniques that then can be compiled in the existing library\nframework, within the user environment."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"When using your software to transfer learn, what about other hyperparameters? Are you just freezing other layers?"})}),"\n",(0,o.jsx)(n.p,{children:"For transfer learning, our tooling allows you to save the sparse architecture learned from larger datasets. Other\nhyperparameters are fully under your control and allow you the flexibility to easily freeze layers as well."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you support INT8 and INT16 (quantized) operations?"})}),"\n",(0,o.jsx)(n.p,{children:"DeepSparse runs at FP32 and has support for INT8.  With Intel Cascade Lake generation chips and later,\nIntel CPUs include VNNI instructions and support both INT8 and INT16 operations. On these machines, performance improvements\nfrom quantization will be greater. DeepSparse has INT8 support for the ONNX operators QLinearConv, QuantizeLinear,\nDequantizeLinear, QLinearMatMul, and MatMulInteger. Our engine also supports 8-bit QLinearAdd, an ONNX Runtime custom operator."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do you support FP16 (half precision) and BF16 operations?"})}),"\n",(0,o.jsx)(n.p,{children:"Neural Magic is looking to include both FP16 and BF16 on our roadmap in the near future."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"runtime-faqs",children:"Runtime FAQs"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Do users have to do any model conversion before using DeepSparse?"})}),"\n",(0,o.jsx)(n.p,{children:"DeepSparse executes on an ONNX (Open Neural Network Exchange) representation of a deep learning model.\nOur software allows you to produce an ONNX representation. If working with PyTorch, we use the built-in ONNX\nexport and for TensorFlow, we convert from a standard exported protobuf file to ONNX. Outside of those frameworks,\nyou would need to convert your model to ONNX first before passing it to DeepSparse."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Why is ONNX the file format used by Neural Magic?"})}),"\n",(0,o.jsx)(n.p,{children:"ONNX (Open Neural Network Exchange) is emerging as a standard, open-source format for model representation.\nBased on the breadth of vendors supporting ONNX as well as the health of open-source community contributions,\nwe believe ONNX offers a compelling solution for the market."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Are your users using ONNX runtime already?"})}),"\n",(0,o.jsx)(n.p,{children:"End users are using a wide variety of runtimes, both open-source and proprietary. Neural Magic is focused on\nensuring we are open and flexible, to allow our users to achieve deep learning performance regardless of how\nthey choose to build, deploy, and run their models."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What is the accuracy loss, if any, on the numbers Neural Magic demonstrates?"})}),"\n",(0,o.jsx)(n.p,{children:"Results will depend on your use case and specific requirements. We are capable of maintaining 100% baseline accuracy.\nIn cases where accuracy is not as important as performance, you can use our model optimization tools to further speed\nup the model at the expense of accuracy and weigh the tradeoffs."}),"\n",(0,o.jsx)(n.p,{children:"If you need sparsification, we provide the tooling for tradeoffs between accuracy and performance based on your specific requirements."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"For the runtime engine, is Neural Magic modifying the architecture in any way or just optimizing the instruction set at that level?"})}),"\n",(0,o.jsx)(n.p,{children:"Specifically for sparsification, our software keeps the architecture intact and changes the weights. For running dense, we do not change anything about the model."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"For a CPU are you using all the cores?"})}),"\n",(0,o.jsxs)(n.p,{children:["DeepSparse optimizes ",(0,o.jsx)(n.em,{children:"how"})," the model is run on the infrastructure resources applied to it. But, Neural\nMagic does not optimize for the number of cores. You are in control to specify how much of the system Neural Magic will use and run on.\nDepending on your goals (latency, throughput, and cost constraints), you can optimize your pipeline for maximum efficiency."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>i});var o=r(6540);const s={},a=o.createContext(s);function t(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);