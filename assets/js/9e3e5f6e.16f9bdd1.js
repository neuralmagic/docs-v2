"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[3314],{1899:e=>{e.exports=JSON.parse('{"label":"performance","permalink":"/docs-v2/tags/performance","allTagsPath":"/docs-v2/tags","count":1,"items":[{"id":"get-started/optimize","title":"Optimizing LLMs","description":"Optimize large language models (LLMs) for efficient inference using one-shot pruning and quantization. Learn how to improve model performance and reduce costs without sacrificing accuracy.","permalink":"/docs-v2/get-started/optimize"}],"unlisted":false}')}}]);