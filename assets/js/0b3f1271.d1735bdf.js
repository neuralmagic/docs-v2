"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[849],{3238:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var t=a(5893),r=a(1151);const i={tags:["finetuning","optimization","llm","sparsezoo","performance","neural magic"],keywords:["LLM finetuning","SparseML","efficient inference","accuracy recovery"],description:"Improve the performance of your large language models (LLMs) through finetuning with Neural Magic's SparseML. Optimize LLMs for specific tasks while maintaining accuracy.",sidebar_label:"Sparse Finetuning",sidebar_position:4},s="Sparse Finetuning with LLMs",l={id:"get-started/finetune",title:"Sparse Finetuning with LLMs",description:"Improve the performance of your large language models (LLMs) through finetuning with Neural Magic's SparseML. Optimize LLMs for specific tasks while maintaining accuracy.",source:"@site/docs/get-started/finetune.mdx",sourceDirName:"get-started",slug:"/get-started/finetune",permalink:"/docs-v2/next/get-started/finetune",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/get-started/finetune.mdx",tags:[{label:"finetuning",permalink:"/docs-v2/next/tags/finetuning"},{label:"optimization",permalink:"/docs-v2/next/tags/optimization"},{label:"llm",permalink:"/docs-v2/next/tags/llm"},{label:"sparsezoo",permalink:"/docs-v2/next/tags/sparsezoo"},{label:"performance",permalink:"/docs-v2/next/tags/performance"},{label:"neural magic",permalink:"/docs-v2/next/tags/neural-magic"}],version:"current",sidebarPosition:4,frontMatter:{tags:["finetuning","optimization","llm","sparsezoo","performance","neural magic"],keywords:["LLM finetuning","SparseML","efficient inference","accuracy recovery"],description:"Improve the performance of your large language models (LLMs) through finetuning with Neural Magic's SparseML. Optimize LLMs for specific tasks while maintaining accuracy.",sidebar_label:"Sparse Finetuning",sidebar_position:4},sidebar:"autogenerated_docs",previous:{title:"Optimize",permalink:"/docs-v2/next/get-started/optimize"},next:{title:"Sparse Transfer",permalink:"/docs-v2/next/get-started/transfer"}},o={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Sparse Finetuning a Llama Model",id:"sparse-finetuning-a-llama-model",level:2},{value:"Data Preparation",id:"data-preparation",level:3},{value:"One Shot Plus Sparse Finetuning",id:"one-shot-plus-sparse-finetuning",level:3},{value:"Inference",id:"inference",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...e.components},{Details:a,TabItem:i,Tabs:s}=n;return a||u("Details",!0),i||u("TabItem",!0),s||u("Tabs",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"sparse-finetuning-with-llms",children:"Sparse Finetuning with LLMs"}),"\n",(0,t.jsx)(n.p,{children:"This guide focuses on improving the performance of large language models (LLMs) after pruning by applying finetuning techniques and then quantizing them for efficient inference.\nYou'll learn how to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Recover Accuracy:"})," Mitigate potential accuracy loss from aggressive pruning by finetuning sparsified models."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Apply Finetuning and Distillation:"})," Leverage knowledge from the original model to refine the sparse model during finetuning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Adapt to Domains:"})," Finetune LLMs to excel in particular domains or use cases."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Training Environment:"})," A system that meets the minimum hardware and software requirements as outlined in the ",(0,t.jsx)(n.a,{href:"./install/#prerequisites",children:"Install Guide"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"SparseML LLM Installation:"})," An environment with DeepSparse for LLMs installed as outlined in the ",(0,t.jsx)(n.a,{href:"./install#llms---causal-language-modeling",children:"Install Guide"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Background:"})," Familiarity with Generative AI and working with large language models is recommended."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"sparse-finetuning-a-llama-model",children:"Sparse Finetuning a Llama Model"}),"\n",(0,t.jsx)(n.p,{children:"We'll use a pre-trained, unoptimized 7b Llama 2 chat model from the SparseZoo trained for instruction tuning utilizing the Open Platypus dataset.\nThe model is referenced by the following SparseZoo stub:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"zoo:llama2-7b-open_platypus_orca_llama2_pretrain-base\n"})}),"\n",(0,t.jsx)(n.p,{children:"For additional models that work with SparseML, consider the following options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Explore pre-sparsified ",(0,t.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?modelSet=generative_ai",children:"Generative AI models in the SparseZoo"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Try out popular LLMs from the ",(0,t.jsx)(n.a,{href:"https://huggingface.co/models?pipeline_tag=causal-lm",children:"Hugging Face Model Hub"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"data-preparation",children:"Data Preparation"}),"\n",(0,t.jsx)(n.p,{children:"SparseML requires a dataset to be used for calibration during the sparsification process.\nFor this example, we'll use the Open Platypus dataset, which is available in the Hugging Face dataset hub and can be loaded as follows:"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)(i,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\n\ndataset = load_dataset("garage-bAInd/Open-Platypus")\n'})})}),(0,t.jsx)(i,{value:"bash",label:"Bash",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'--dataset "garage-bAInd/Open-Platypus"\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["For comprehensive data preparation guidelines, including formats like CSV and JSONL, refer to our ",(0,t.jsx)(n.a,{href:"../llms/data",children:"detailed datasets guide"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"one-shot-plus-sparse-finetuning",children:"One Shot Plus Sparse Finetuning"}),"\n",(0,t.jsxs)(n.p,{children:["Applying pruning in one shot to an LLM can result in drops in accuracy at higher sparsity levels.\nTo mitigate this, SparseML can apply a recipe that includes pruning and finetuning.\nFinetuning the model after pruning will help to recover any lost accuracy and improve the model's performance at higher sparsity levels.\nAs in the one-shot guide, we'll apply the SparseGPT algorithm and the ",(0,t.jsx)(n.code,{children:"compress"})," command in SparseML to a trained model."]}),"\n",(0,t.jsx)(n.p,{children:"The code below demonstrates applying one-shot pruning, followed by finetuning, and finally, one-shot quantization to the Llama model utilizing a recipe:"}),"\n",(0,t.jsx)(s,{children:(0,t.jsx)(i,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import (\n    SparseAutoModelForCausalLM, SparseAutoTokenizer, load_dataset, compress\n)\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-base",\n    device_map="auto",\n)\ntokenizer = SparseAutoTokenizer.from_pretrained(\n    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-base"\n)\ndataset = load_dataset("garage-bAInd/Open-Platypus")\n\ndef format_data(data):\n    return {\n        "text": data["instruction"] + data["output"]\n    }\n\ndataset = dataset.map(format_data)\n\nrecipe = """\npruning_stage:\n    run_type: oneshot\n    pruning_modifiers:\n        SparseGPTModifier:\n            sparsity: 0.5\n            quantize: False\n            targets: [model.layers.0, model.layers.1, model.layers.2, model.layers.3, model.layers.4, model.layers.5, model.layers.6, model.layers.7, model.layers.8, model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, lm_head]\n\nfinetune_stage:\n    run_type: train\n    finetune_modifiers:\n        ConstantPruningModifier:\n            targets: ["re:.*q_proj.weight", "re:.*k_proj.weight", "re:.*v_proj.weight", "re:.*o_proj.weight", "re:.*gate_proj.weight", "re:.*up_proj.weight", "re:.*down_proj.weight"]\n            start: 0\n\nquantization_stage:\n    run_type: oneshot\n    quantize_modifiers:\n        QuantizationModifier:\n            ignore: [LlamaRotaryEmbedding, LlamaRMSNorm, SiLUActivation, MatMulOutput_QK, MatMulOutput_PV]\n            post_oneshot_calibration: true\n            scheme_overrides:\n                Linear:\n                    weights:\n                        num_bits: 8\n                        symmetric: true\n                        strategy: channel\n                MatMulLeftInput_QK:\n                    input_activations:\n                        num_bits: 8\n                        symmetric: true\n                Embedding:\n                    input_activations: null\n                    weights:\n                        num_bits: 8\n                        symmetric: false\n        SparseGPTModifier:\n            quantize: True\n            targets: [model.layers.0, model.layers.1, model.layers.2, model.layers.3, model.layers.4, model.layers.5, model.layers.6, model.layers.7, model.layers.8, model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, lm_head]\n"""\n\ncompress(\n    model=model,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    recipe=recipe,\n    output_dir="./finetune-example"\n)\n'})})})}),"\n",(0,t.jsx)(n.p,{children:"After running the above code, the model is pruned to 50% sparsity, fine-tuned, and quantized, resulting in a smaller model ready for efficient inference."}),"\n",(0,t.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,t.jsxs)(a,{children:[(0,t.jsx)("summary",{children:"Evaluating Accuracy"}),(0,t.jsx)(n.p,{children:"Evaluating the model's accuracy is important to ensure it meets the desired performance requirements.\nTo do so, we can use the following code to evaluate the model's perplexity on a sample dataset:"}),(0,t.jsxs)(s,{children:[(0,t.jsx)(i,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import evaluate\n\neval = evaluate(\n    "./finetune-example/quantization_stage",\n    datasets="openai_humaneval",\n    integration="perplexity",\n    text_column_name=["prompt", "canonical_solution"]\n)\nprint(eval)\n'})})}),(0,t.jsx)(i,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.evaluate \\\n    "./finetune-example/quantization_stage" \\\n    --datasets "openai_humaneval" \\\n    --integration "perplexity" \\\n    --text_column_name prompt \\\n    --text_column_name canonical_solution\n'})})})]})]}),"\n",(0,t.jsx)(n.p,{children:"After sparsifying the model, it is ready for evaluation and deployment.\nTo test the model's generation capabilities, we can use the following code to generate text utilizing PyTorch:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer\n\nmodel_path = "./finetune-example/quantization_stage"\nmodel = SparseAutoModelForCausalLM.from_pretrained(model_path, device_map="auto")\ntokenizer = SparseAutoTokenizer.from_pretrained(model_path)\ninputs = tokenizer(["Large language models are"], return_tensors="pt")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(outputs)\n'})}),"\n",(0,t.jsx)(n.p,{children:"The above code, however, does not leverage the sparsity within the model for efficient inference.\nTo do so, we need to export the model to ONNX to be ready for efficient inference on CPUs with DeepSparse.\nSparseML provides a simple export command to do so:"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)(i,{value:"python",label:"Python",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sparseml import export\n\nexport(\n    "./finetune-example/quantization_stage",\n    task="text-generation",\n    sequence_length=1024,\n    target_path="./exported"\n)\n'})})}),(0,t.jsx)(i,{value:"bash",label:"Bash",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sparseml.export \\\n    "./finetune-example/quantization_stage" \\\n    --task "text-generation" \\\n    --sequence_length 1024 \\\n    --target_path "./exported"\n'})})})]}),"\n",(0,t.jsxs)(n.p,{children:["The exported model located at ",(0,t.jsx)(n.code,{children:"./exported"})," can now be used for efficient inference with DeepSparse.\nTo do so, sub in the exported model within the previous ",(0,t.jsx)(n.a,{href:"./deploy",children:"Getting Started - Deploy"})," guide for your desired deployment method."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"Want to dive into more about one-shot sparsification with Neural Magic? Here are a few paths to consider:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Specialize in LLMs:"})," Dive deeper into text generation techniques within our ",(0,t.jsx)(n.a,{href:"../llms",children:"LLMs section"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Expand to Other Domains:"})," Explore how to optimize models for ",(0,t.jsx)(n.a,{href:"../computer-vision",children:"Computer Vision"})," or ",(0,t.jsx)(n.a,{href:"../nlp",children:"Natural Language Processing"})," tasks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)("b",{children:"Tailor to Your Needs:"})," Learn about flexible sparsification options in our ",(0,t.jsx)(n.a,{href:"../custom-integrations",children:"Custom Integrations section"}),"."]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}function u(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},1151:(e,n,a)=>{a.d(n,{Z:()=>l,a:()=>s});var t=a(7294);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);