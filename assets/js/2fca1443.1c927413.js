"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[430],{4667:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=r(5893),t=r(1151);const i={title:"Serving LLMs",sidebar_position:1},a="Deploy LLMs with DeepSparse",o={id:"serving-llms",title:"Serving LLMs",description:"DeepSparse is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with SparseML, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.",source:"@site/docs/serving-llms.md",sourceDirName:".",slug:"/serving-llms",permalink:"/docs-v2/serving-llms",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/serving-llms.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Serving LLMs",sidebar_position:1},sidebar:"autogenerated_docs",previous:{title:"Sparse Finetuning",permalink:"/docs-v2/get-started/finetune"},next:{title:"Guides",permalink:"/docs-v2/guides/"}},l={},c=[{value:"Supported LLM Architectures",id:"supported-llm-architectures",level:3},{value:"Making new DeepSparse-optimized models",id:"making-new-deepsparse-optimized-models",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"deploy-llms-with-deepsparse",children:"Deploy LLMs with DeepSparse"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/neuralmagic/deepsparse",children:"DeepSparse"})," is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with ",(0,s.jsx)(n.a,{href:"https://github.com/neuralmagic/sparseml",children:"SparseML"}),", our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware."]}),"\n",(0,s.jsx)(n.p,{children:"Neural Magic supports performant LLM inference in DeepSparse with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sparse kernels for faster inference and memory savings from unstructured sparse weights"}),"\n",(0,s.jsx)(n.li,{children:"8-bit weight and activation quantization support"}),"\n",(0,s.jsx)(n.li,{children:"Efficient management of cached attention keys and values for minimal latency"}),"\n",(0,s.jsx)(n.li,{children:"Continuous batching to optimize output tokens generation throughput"}),"\n",(0,s.jsx)(n.li,{children:"Streaming outputs"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI-compatible API server"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://github.com/neuralmagic/deepsparse/assets/3195154/ccf39323-4603-4489-8462-7b103872aeb3",alt:"mpt-chat-comparison"})}),"\n",(0,s.jsxs)(n.p,{children:["Here's a minimal example showing how to use DeepSparse for text generation with ",(0,s.jsx)(n.code,{children:"TinyStories-1M"}),", a very small model for generating stories."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from deepsparse import TextGeneration\npipeline = TextGeneration(model="hf:mgoin/TinyStories-1M-ds")\nprint(pipeline("Once upon a time, ").generations[0].text)\n"""\nOne day, a little girl named Lily went to the park with her mommy. They saw a big slide and wanted to slide down the slide. Lily said, "Mommy, can I go on the slide?" Her mommy said, "Yes, you can go on the slide."\n"""\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["Check out the [",(0,s.jsx)(n.code,{children:"TextGeneration"})," documentation for usage details.](ADD LINK)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"supported-llm-architectures",children:"Supported LLM Architectures"}),"\n",(0,s.jsx)(n.p,{children:"DeepSparse supports many Hugging Face models through [ONNX export through SparseML][ADD LINK], include the following architectures:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["LLaMA & LLaMA-2 - ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/Llama2-7b-chat-pruned50-quant-ds",children:"neuralmagic/Llama2-7b-chat-pruned50-quant-ds"}),", ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/Nous-Hermes-llama-2-7b-pruned50-quant-ds",children:"neuralmagic/Nous-Hermes-llama-2-7b-pruned50-quant-ds"}),", ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds",children:"neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds"})]}),"\n",(0,s.jsxs)(n.li,{children:["Mistral - ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50-quant-ds",children:"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50-quant-ds"})," - ",(0,s.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?architectures=mistral&ungrouped=true",children:"SparseZoo Models"})]}),"\n",(0,s.jsxs)(n.li,{children:["MPT - ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/mpt-7b-chat-pruned50-quant-ds",children:"neuralmagic/mpt-7b-chat-pruned50-quant-ds"})," - ",(0,s.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?architectures=mpt&ungrouped=true",children:"SparseZoo Models"})]}),"\n",(0,s.jsxs)(n.li,{children:["OPT - facebook/opt-6.7b, etc. - ",(0,s.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?architectures=opt&ungrouped=true",children:"SparseZoo Models"})]}),"\n",(0,s.jsxs)(n.li,{children:["SOLAR - ",(0,s.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/Nous-Hermes-2-SOLAR-10.7B-pruned50-quant-ds",children:"neuralmagic/Nous-Hermes-2-SOLAR-10.7B-pruned50-quant-ds"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"making-new-deepsparse-optimized-models",children:"Making new DeepSparse-optimized models"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"/docs-v2/guides/one-shot-llms-with-sparseml",children:"See the guide for compressing LLMs with SparseGPT"})})]})}function p(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,n,r)=>{r.d(n,{Z:()=>o,a:()=>a});var s=r(7294);const t={},i=s.createContext(t);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);