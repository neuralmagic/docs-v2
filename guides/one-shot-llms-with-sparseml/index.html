<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-1.7.0 docs-doc-page docs-doc-id-guides/one-shot-llms-with-sparseml" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">Compress LLMs with SparseGPT | Neural Magic Documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://neuralmagic.github.io/docs-v2/guides/one-shot-llms-with-sparseml"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="1.7.0"><meta data-rh="true" name="docusaurus_tag" content="docs-default-1.7.0"><meta data-rh="true" name="docsearch:version" content="1.7.0"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-1.7.0"><meta data-rh="true" property="og:title" content="Compress LLMs with SparseGPT | Neural Magic Documentation"><meta data-rh="true" name="description" content="This page describes how to perform one-shot quantization of large language models using SparseML. This workflow requires a GPU with at least 16GB VRAM and 64GB of system RAM."><meta data-rh="true" property="og:description" content="This page describes how to perform one-shot quantization of large language models using SparseML. This workflow requires a GPU with at least 16GB VRAM and 64GB of system RAM."><link data-rh="true" rel="icon" href="/docs-v2/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://neuralmagic.github.io/docs-v2/guides/one-shot-llms-with-sparseml"><link data-rh="true" rel="alternate" href="https://neuralmagic.github.io/docs-v2/guides/one-shot-llms-with-sparseml" hreflang="en"><link data-rh="true" rel="alternate" href="https://neuralmagic.github.io/docs-v2/guides/one-shot-llms-with-sparseml" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://CIS4HPXHOK-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="Neural Magic Documentation" href="/docs-v2/opensearch.xml"><link rel="stylesheet" href="/docs-v2/assets/css/styles.23a16109.css">
<script src="/docs-v2/assets/js/runtime~main.4acc2304.js" defer="defer"></script>
<script src="/docs-v2/assets/js/main.e0ce076c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs-v2/"><div class="navbar__logo"><img src="/docs-v2/img/logo.svg" alt="Neural Magic Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/docs-v2/img/logo.svg" alt="Neural Magic Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Docs</b></a><div class="navbar__item dropdown dropdown--hoverable"><a aria-current="page" class="navbar__link active" aria-haspopup="true" aria-expanded="false" role="button" href="/docs-v2/">1.7.0</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs-v2/next/guides/one-shot-llms-with-sparseml">nightly</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs-v2/guides/one-shot-llms-with-sparseml">1.7.0</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">GitHub</a><ul class="dropdown__menu"><li><a href="https://github.com/neuralmagic/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Neural Magic</a></li><li><a href="https://github.com/neuralmagic/docs" target="_blank" rel="noopener noreferrer" class="dropdown__link">Docs</a></li><li><a href="https://github.com/neuralmagic/deepsparse" target="_blank" rel="noopener noreferrer" class="dropdown__link">DeepSparse</a></li><li><a href="https://github.com/neuralmagic/sparseml" target="_blank" rel="noopener noreferrer" class="dropdown__link">SparseML</a></li><li><a href="https://github.com/neuralmagic/sparsezoo" target="_blank" rel="noopener noreferrer" class="dropdown__link">SparseZoo</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Products</a><ul class="dropdown__menu"><li><a href="https://neuralmagic.com/deepsparse" target="_blank" rel="noopener noreferrer" class="dropdown__link">DeepSparse</a></li><li><a href="https://neuralmagic.com/sparseml" target="_blank" rel="noopener noreferrer" class="dropdown__link">SparseML</a></li><li><a href="https://sparsezoo.neuralmagic.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">SparseZoo</a></li><li><a href="http://neuralmagic.com/labs" target="_blank" rel="noopener noreferrer" class="dropdown__link">Labs by Neural Magic</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Resources</a><ul class="dropdown__menu"><li><a href="https://neuralmagic.com/blog/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Blog</a></li><li><a href="https://neuralmagic.com/support" target="_blank" rel="noopener noreferrer" class="dropdown__link">Support</a></li><li><a href="http://neuralmagic.com/resources/technical-papers/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Research Papers</a></li><li><a href="http://neuralmagic.com/neural-magic-events/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Upcoming Events</a></li><li><a href="http://neuralmagic.com/video" target="_blank" rel="noopener noreferrer" class="dropdown__link">NeuralFlix</a></li><li><a href="http://neuralmagic.com/technology" target="_blank" rel="noopener noreferrer" class="dropdown__link">Our Technology</a></li><li><a href="http://neuralmagic.com/deep-sparse-community" target="_blank" rel="noopener noreferrer" class="dropdown__link">Subscribe</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Company</a><ul class="dropdown__menu"><li><a href="http://neuralmagic.com/about" target="_blank" rel="noopener noreferrer" class="dropdown__link">Team &amp; Investors</a></li><li><a href="https://apply.workable.com/neural-magic/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Careers</a></li><li><a href="http://neuralmagic.com/contact" target="_blank" rel="noopener noreferrer" class="dropdown__link">Contact</a></li><li><a href="http://neuralmagic.com/legal" target="_blank" rel="noopener noreferrer" class="dropdown__link">Legal</a></li></ul></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_BF9v"><div class="docsSideNavBackground_MOrL"></div><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_Rg9N"><aside class="theme-doc-sidebar-container docSidebarContainer_tLwi"><div class="sidebarViewport_wp6o"><div class="sidebar_mhZE"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_Y1UP"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs-v2/">Home</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="true" href="/docs-v2/get-started/">Getting Started</a><button aria-label="Collapse sidebar category &#x27;Getting Started&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs-v2/get-started/install/">Install</a><button aria-label="Expand sidebar category &#x27;Install&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/get-started/deploy">Deploy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/get-started/optimize">Optimize</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/get-started/finetune">Sparse Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/get-started/transfer">Sparse Transfer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs-v2/guides/">Guides</a><button aria-label="Collapse sidebar category &#x27;Guides&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/guides/why-weight-sparsity">Why is Sparsity Important for LLMs?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/guides/hf-llm-to-deepsparse">Convert LLMs from HF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs-v2/guides/one-shot-llms-with-sparseml">Compress LLMs with SparseGPT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs-v2/guides/llm-serving-on-windows">LLM Serving on Windows</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs-v2/llms/">LLMs - Causal Language Modeling</a><button aria-label="Expand sidebar category &#x27;LLMs - Causal Language Modeling&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs-v2/computer-vision/">Computer Vision</a><button aria-label="Expand sidebar category &#x27;Computer Vision&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs-v2/nlp/">Natural Language Processing</a><button aria-label="Expand sidebar category &#x27;Natural Language Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs-v2/products/">Products</a><button aria-label="Expand sidebar category &#x27;Products&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs-v2/details/">Details</a><button aria-label="Expand sidebar category &#x27;Details&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_Vlkt"><div class="container docItemWrapper_GJ0o"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs-v2/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs-v2/guides/"><span itemprop="name">Guides</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Compress LLMs with SparseGPT</span><meta itemprop="position" content="2"></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">Version: 1.7.0</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Compress LLMs with SparseGPT</h1>
<p>This page describes how to perform one-shot quantization of large language models using <a href="https://github.com/neuralmagic/sparseml" target="_blank" rel="noopener noreferrer">SparseML</a>. This workflow requires a GPU with at least 16GB VRAM and 64GB of system RAM.</p>
<blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="note-on-system-requirements">Note on system requirements<a href="#note-on-system-requirements" class="hash-link" aria-label="Direct link to Note on system requirements" title="Direct link to Note on system requirements">​</a></h3>
<p>Due to inefficiencies in PyTorch ONNX export, a lot of system memory is required to export the models for inference. There are <a href="https://github.com/pytorch/pytorch/commit/b4a49124c8165a374a3ef49e14807ac05b3fc030" target="_blank" rel="noopener noreferrer">improvements coming in 2.2</a>.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-clone-and-install-the-latest-sparseml">How to Clone and Install the Latest SparseML<a href="#how-to-clone-and-install-the-latest-sparseml" class="hash-link" aria-label="Direct link to How to Clone and Install the Latest SparseML" title="Direct link to How to Clone and Install the Latest SparseML">​</a></h2>
<p>You&#x27;ll need the latest version of SparseML to run the one-shot workflow. We recommend that you do this from source and in a fresh Python environment to avoid any issues.</p>
<p>Clone the SparseML repo and install it locally:</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token function" style="color:rgb(220, 220, 170)">git</span><span class="token plain"> clone https://github.com/neuralmagic/sparseml</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">pip </span><span class="token function" style="color:rgb(220, 220, 170)">install</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">-e</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;sparseml[transformers]&quot;</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-one-shot-tinyllama">How to One-shot TinyLlama<a href="#how-to-one-shot-tinyllama" class="hash-link" aria-label="Direct link to How to One-shot TinyLlama" title="Direct link to How to One-shot TinyLlama">​</a></h2>
<p><a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4" target="_blank" rel="noopener noreferrer">TinyLlama-1.1B-Chat</a> is an LLM that we can quantize in a short time because it has 1.1B parameters.</p>
<p>Perform one-shot using the OBCQ algorithm. The command takes the following parameters:</p>
<p>positional arguments:</p>
<ul>
<li><code>model_name</code> a path to Hugging Face stub</li>
<li><code>dataset_name</code> Hugging Face dataset to extract calibration data from. Example of supported datasets: <code>{c4,evolcodealpaca,gsm8k,open_platypus,ptb,wikitext2}</code></li>
</ul>
<p>options:</p>
<ul>
<li><code>--dataset_config_name</code> Specific configuration to extract from the dataset, i.e. <code>wikitext2-raw-v1</code> for <code>wikitext2</code></li>
<li><code>--nsamples</code> number of samples to extract from the dataset, defaults to 512.</li>
<li><code>--seqlen</code> Maximum input sequence length to truncate calibration data to, defaults to model&#x27;s max sequence length</li>
<li><code>--concat_data</code> Whether or not to concatenate samples to fill the full seqlen, defaults to False</li>
<li><code>--output_dir</code> the directory where the model will be saved, defaults to <code>obcq_deployment</code>.</li>
<li><code>--recipe</code> the file containing the one-shot hyperparameters.</li>
<li><code>--device</code> which device to load the model onto, either <code>cpu</code> or a specific <code>cuda:0</code>.</li>
<li><code>--precision</code> precision to load model as, either auto (default), half, full, float16 or float32.</li>
</ul>
<p>Example command:</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token function" style="color:rgb(220, 220, 170)">wget</span><span class="token plain"> https://huggingface.co/nm-testing/TinyLlama-1.1B-Chat-v0.4-pruned50-quant/raw/main/recipe.yaml </span><span class="token comment" style="color:rgb(106, 153, 85)"># download recipe</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">sparseml.transformers.text_generation.oneshot </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--model_name</span><span class="token plain"> TinyLlama/TinyLlama-1.1B-Chat-v1.0 </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--dataset_name</span><span class="token plain"> open_platypus </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--recipe</span><span class="token plain"> recipe.yaml </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--output_dir</span><span class="token plain"> ./obcq_deployment </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--precision</span><span class="token plain"> float16</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-evaluate-the-one-shot-model">How to Evaluate the One-shot Model<a href="#how-to-evaluate-the-one-shot-model" class="hash-link" aria-label="Direct link to How to Evaluate the One-shot Model" title="Direct link to How to Evaluate the One-shot Model">​</a></h2>
<p>Next, evaluate the model&#x27;s performance using the <a href="https://github.com/neuralmagic/lm-evaluation-harness" target="_blank" rel="noopener noreferrer">lm-evaluation-harness framework</a>.</p>
<p>Clone the forked repository with SparseML support and install it:</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token function" style="color:rgb(220, 220, 170)">git</span><span class="token plain"> clone https://github.com/neuralmagic/lm-evaluation-harness.git</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(78, 201, 176)">cd</span><span class="token plain"> lm-evaluation-harness</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">pip </span><span class="token function" style="color:rgb(220, 220, 170)">install</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">-e</span><span class="token plain"> </span><span class="token builtin class-name" style="color:rgb(78, 201, 176)">.</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Evaluate on the <code>hellaswag</code> task:</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token assign-left variable" style="color:rgb(156, 220, 254)">start</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token variable function" style="color:rgb(220, 220, 170)">date</span><span class="token variable" style="color:rgb(156, 220, 254)"> +%s</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">python main.py </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--model</span><span class="token plain"> hf-causal-experimental </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--model_args</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(156, 220, 254)">pretrained</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain">obcq_deployment,trust_remote_code</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain">True </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--tasks</span><span class="token plain"> hellaswag </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--batch_size</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">64</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--no_cache</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--write_out</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--output_path</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;obcq_deployment/hellaswag.json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--device</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;cuda:0&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--num_fewshot</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(156, 220, 254)">end</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token variable function" style="color:rgb(220, 220, 170)">date</span><span class="token variable" style="color:rgb(156, 220, 254)"> +%s</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"> </span><span class="token builtin class-name" style="color:rgb(78, 201, 176)">echo</span><span class="token plain"> Execution </span><span class="token function" style="color:rgb(220, 220, 170)">time</span><span class="token plain"> was </span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token variable function" style="color:rgb(220, 220, 170)">expr</span><span class="token variable" style="color:rgb(156, 220, 254)"> $end - $start</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token plain"> seconds.</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The results obtained in this case are:</p>
<div class="codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-text codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token plain">Running loglikelihood requests</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">100%|██████████| 40145/40145 [20:47&lt;00:00, 32.19it/s] </span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">{</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  &quot;results&quot;: {</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;hellaswag&quot;: {</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;acc&quot;: 0.40141406094403503,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;acc_stderr&quot;: 0.004891826692722827,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;acc_norm&quot;: 0.5115514837681737,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;acc_norm_stderr&quot;: 0.004988449593007253</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  &quot;versions&quot;: {</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;hellaswag&quot;: 0</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  &quot;config&quot;: {</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;model&quot;: &quot;hf-causal-experimental&quot;,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;model_args&quot;: {</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;pretrained&quot;: &quot;/home/mwitiderrick/neuralmagic/sparseml/obcq_deployment&quot;,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      &quot;trust_remote_code&quot;: true</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;num_fewshot&quot;: 0,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;batch_size&quot;: &quot;64&quot;,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;batch_sizes&quot;: [],</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;device&quot;: &quot;cuda:0&quot;,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;no_cache&quot;: true,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;limit&quot;: null,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;bootstrap_iters&quot;: 100000,</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    &quot;description_dict&quot;: {}</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">}</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">hf-causal-experimental (pretrained=/home/mwitiderrick/neuralmagic/sparseml/obcq_deployment,trust_remote_code=True), limit: None, provide_description: False, num_fewshot: 0, batch_size: 64</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">| Task      | Version | Metric   |  Value |     | Stderr |</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">| --------- | ------: | -------- | -----: | --- | -----: |</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">| hellaswag |       0 | acc      | 0.4014 | ±   | 0.0049 |</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">|           |         | acc_norm | 0.5116 | ±   | 0.0050 |</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">Execution time was 1288 seconds.</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Repeat the above on other tasks such as <code>truthfulqa-mc</code>, <code>winogrande</code>, and <code>drop</code>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-export-the-one-shot-model">How to Export the One-shot Model<a href="#how-to-export-the-one-shot-model" class="hash-link" aria-label="Direct link to How to Export the One-shot Model" title="Direct link to How to Export the One-shot Model">​</a></h2>
<p>Once you are certain the model is performing as expected, you can export it for inference. The <code>sparseml.export</code> command provides the functions for doing this. Running the command below creates a <code>deployment</code> directory containing all the artifacts that are needed for inference with DeepSparse. It will also inject KV Cache to reduce the model’s computational overhead and speed up inference by caching the Key and Value states.</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token plain">sparseml.export </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--task</span><span class="token plain"> text-generation obcq_deployment/</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="using-the-model-with-deepsparse">Using the Model With DeepSparse<a href="#using-the-model-with-deepsparse" class="hash-link" aria-label="Direct link to Using the Model With DeepSparse" title="Direct link to Using the Model With DeepSparse">​</a></h2>
<p>Next, run inference using DeepSparse. Ensure you have the latest version of DeepSparse installed with <code>pip install -U deepsparse-nightly[llm]</code></p>
<div class="language-python codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-python codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token keyword" style="color:rgb(86, 156, 214)">from</span><span class="token plain"> deepsparse </span><span class="token keyword" style="color:rgb(86, 156, 214)">import</span><span class="token plain"> TextGeneration</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;How to get in a good university?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">formatted_prompt </span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain">  </span><span class="token string-interpolation string" style="color:rgb(206, 145, 120)">f&quot;&lt;|im_start|&gt;user\n</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token string-interpolation interpolation">prompt</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token string-interpolation string" style="color:rgb(206, 145, 120)">&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">model </span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain"> TextGeneration</span><span class="token punctuation" style="color:rgb(212, 212, 212)">(</span><span class="token plain">model</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;obcq_deployment/deployment&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token keyword" style="color:rgb(86, 156, 214)">print</span><span class="token punctuation" style="color:rgb(212, 212, 212)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(212, 212, 212)">(</span><span class="token plain">formatted_prompt</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> max_new_tokens</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token number" style="color:rgb(181, 206, 168)">200</span><span class="token punctuation" style="color:rgb(212, 212, 212)">)</span><span class="token punctuation" style="color:rgb(212, 212, 212)">.</span><span class="token plain">generations</span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token number" style="color:rgb(181, 206, 168)">0</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">.</span><span class="token plain">text</span><span class="token punctuation" style="color:rgb(212, 212, 212)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">There are many factors to consider when choosing a university. Here are some tips for getting into a good university:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">1. Research your options: Consider the schools in your area and the ones in your desired location. Research their reputation, tuition, and academic programs.</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">2. Apply to multiple universities: Apply to multiple universities, ensuring that you are applying to the best option for you.</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">3. Get a job: If you are applying to a university, you will need to find a job to support your studies. This will help you budget and manage your time.</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">4. Get involved with your community: Your university will likely have a community of students and faculty. Engage with this community by volunteering, participating in clubs, and engaging with others in your community.</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">5. Get involved with extracurricular activities: Universities often have many extracurricular activities, which can help you meet new people</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token triple-quoted-string string" style="color:rgb(206, 145, 120)">&quot;&quot;&quot;</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Check out the <a href="https://github.com/neuralmagic/deepsparse/blob/main/src/deepsparse/transformers/text_generation.md" target="_blank" rel="noopener noreferrer">DeepSparse pipeline text generation docs</a> for the full list of supported parameters.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="upload-model-to-hugging-face">Upload Model to Hugging Face<a href="#upload-model-to-hugging-face" class="hash-link" aria-label="Direct link to Upload Model to Hugging Face" title="Direct link to Upload Model to Hugging Face">​</a></h2>
<p>You may want to upload the one-shot model to Hugging Face for ease of reference or to share it with your colleagues.</p>
<p>Head over to your <a href="https://huggingface.co/new" target="_blank" rel="noopener noreferrer">Hugging Face account</a> and create a model named <code>TinyLlama-1.1B-Chat-v0.4-pruned50-quant</code>. Then upload the one-shot model:</p>
<div class="language-python codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-python codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token keyword" style="color:rgb(86, 156, 214)">from</span><span class="token plain"> huggingface_hub </span><span class="token keyword" style="color:rgb(86, 156, 214)">import</span><span class="token plain"> HfApi</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">api </span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token plain"> HfApi</span><span class="token punctuation" style="color:rgb(212, 212, 212)">(</span><span class="token punctuation" style="color:rgb(212, 212, 212)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">api</span><span class="token punctuation" style="color:rgb(212, 212, 212)">.</span><span class="token plain">upload_folder</span><span class="token punctuation" style="color:rgb(212, 212, 212)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    folder_path</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;deployment&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    repo_id</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;YOUR_HF_USERNAME/TinyLlama-1.1B-Chat-v0.4-pruned50-quant&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    repo_type</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    token</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;HF_WRITE_TOKEN&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">)</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="explaining-the-tinyllama-recipe">Explaining the TinyLlama Recipe<a href="#explaining-the-tinyllama-recipe" class="hash-link" aria-label="Direct link to Explaining the TinyLlama Recipe" title="Direct link to Explaining the TinyLlama Recipe">​</a></h2>
<p>A recipe is a set of hyperparameters that provide detailed instructions on how the <a href="https://neuralmagic.com/video/pruning-and-quantizing-ml-models-with-one-shot-without-retraining/" target="_blank" rel="noopener noreferrer">one-shot quantization</a> should be done. The recipe performs quantization in one shot, meaning that no retraining of the LLM is required.</p>
<p>We will now walk through what the different hyperparameters mean and why they are set to those values.</p>
<p>The <code>SmoothQuantModifier</code> is a technique used for dealing with outliers in the weights and activations of the LLM because quantization is very sensitive to large variations in their values. For TinyLlama a <code>smoothing_strength</code> value of 0.8 resulted in a model with repetitions in its output but the problem was solved by lowering the value to 0.5.</p>
<p>The <code>ignore</code> parameter under <code>QuantizationModifier</code> allows us to define operations that either don&#x27;t make sense to quantize or operations that are too sensitive to quantize. Performing quantization on sensitive operations will affect the final accuracy of the model. We also don&#x27;t quantize the inputs to the embedding layer.</p>
<p>Under <code>SparseGPTModifier</code>, we define <code>sparsity</code> as 0.5 because we are aiming for a model that is 50% quantized. The other parameters are:</p>
<ul>
<li><code>block_size</code> determines the number of columns to compress in one pass.</li>
<li><code>quantize</code> whether or not to quantize weights during SparseGPT.  A default quantization modifier will be applied when <code>quantize</code> is set to <code>True</code> and there is no <code>QuantizationModifier</code> in the recipe.</li>
<li><code>dampening_frac</code> amount of dampening to apply to H, as a fraction of the diagonal norm.</li>
<li><code>sequential_update</code> whether or not to update weights sequentially by layer, True saves on GPU memory.</li>
<li><code>mask_structure</code> string to define the structure of the mask to apply, &quot;0:0&quot; means that it&#x27;s an unstructured mask. Setting it to &quot;16:32&quot; would mean that 16 out of every 32 weights will be zeroed out (structured sparsity).</li>
<li><code>targets</code> list of layer names to compress during OBCQ, or &#x27;<strong>ALL</strong>&#x27; to compress every layer in the model.</li>
</ul>
<div class="language-yaml codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-yaml codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token key atrule">test_stage</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">obcq_modifiers</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">SmoothQuantModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">smoothing_strength</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">mappings</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*q_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*k_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*v_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*input_layernorm&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*gate_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*up_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*post_attention_layernorm&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">QuantizationModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">ignore</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token comment" style="color:rgb(106, 153, 85)"># These operations don&#x27;t make sense to quantize</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> LlamaRotaryEmbedding</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> LlamaRMSNorm</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> SiLUActivation</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token comment" style="color:rgb(106, 153, 85)"># Skip quantizing the BMMs</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> QuantizableMatMul</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token comment" style="color:rgb(106, 153, 85)"># Skip quantizing the layers with the most sensitive activations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.21.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.7.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.2.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.20.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.19.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">post_oneshot_calibration</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">scheme_overrides</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token key atrule">Embedding</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">          </span><span class="token key atrule">input_activations</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token null important">null</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">          </span><span class="token key atrule">weights</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            </span><span class="token key atrule">num_bits</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            </span><span class="token key atrule">symmetric</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">SparseGPTModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">sparsity</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">block_size</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">sequential_update</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">quantize</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">percdamp</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.01</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">mask_structure</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;0:0&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">targets</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:model.layers.\\d*$&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-adapt-a-recipe-for-a-new-model">How to Adapt a Recipe for a New Model<a href="#how-to-adapt-a-recipe-for-a-new-model" class="hash-link" aria-label="Direct link to How to Adapt a Recipe for a New Model" title="Direct link to How to Adapt a Recipe for a New Model">​</a></h2>
<p>You can modify the above recipe to perform one-shot quantization on other models, for example <a href="https://huggingface.co/docs/transformers/main/model_doc/mistral" target="_blank" rel="noopener noreferrer">Mistral</a>.</p>
<p>Perform the following modifications on the recipe to one-shot a Mistral model.</p>
<ul>
<li>Define the operations we want to skip during quantization, that is sensitive layers and operations that don&#x27;t make sense to quantize.</li>
<li>Declare the desired sparsity level, same as the one for TinyLlama.</li>
<li>State the layers to compress during OBCQ.</li>
</ul>
<p>Here is what the final recipe looks like:</p>
<div class="language-yaml codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-yaml codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token key atrule">test_stage</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">obcq_modifiers</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">SmoothQuantModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">smoothing_strength</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">mappings</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*q_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*k_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*v_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*input_layernorm&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*gate_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*up_proj&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:.*post_attention_layernorm&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">QuantizationModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">ignore</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token comment" style="color:rgb(106, 153, 85)"># These operations don&#x27;t make sense to quantize</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> MistralRotaryEmbedding</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> MistralRMSNorm</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> SiLUActivation</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token comment" style="color:rgb(106, 153, 85)"># Skip quantizing the layers with the most sensitive activations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.1.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.31.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.30.mlp.down_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.30.mlp.gate_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> model.layers.30.mlp.up_proj</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">post_oneshot_calibration</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">scheme_overrides</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token key atrule">Embedding</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">          </span><span class="token key atrule">input_activations</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token null important">null</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">          </span><span class="token key atrule">weights</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            </span><span class="token key atrule">num_bits</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            </span><span class="token key atrule">symmetric</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">SparseGPTModifier</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">sparsity</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">block_size</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">sequential_update</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">quantize</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">percdamp</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0.01</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">mask_structure</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;0:0&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      </span><span class="token key atrule">targets</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(212, 212, 212)">[</span><span class="token string" style="color:rgb(206, 145, 120)">&quot;re:model.layers.\\d*$&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">]</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Save the recipe to a file named <code>recipe.yaml</code>.</p>
<p>Run one-shot quantization on any Mistral-based model, for example, <code>zephyr-7b-beta</code>:</p>
<div class="language-bash codeBlockContainer_APcc theme-code-block" style="--prism-color:#9CDCFE;--prism-background-color:#1E1E1E"><div class="codeBlockContent_m3Ux"><pre tabindex="0" class="prism-code language-bash codeBlock_qGQc thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_p187"><span class="token-line" style="color:#9CDCFE"><span class="token plain">sparseml.transformers.text_generation.oneshot </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--model_name</span><span class="token plain"> HuggingFaceH4/zephyr-7b-beta </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--dataset_name</span><span class="token plain"> open_platypus </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--recipe</span><span class="token plain"> recipe.yaml </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--output_dir</span><span class="token plain"> ./output_oneshot </span><span class="token parameter variable" style="color:rgb(156, 220, 254)">--precision</span><span class="token plain"> float16</span><br></span></code></pre><div class="buttonGroup_6DOT"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_FhaS" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_phi_"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_FfTR"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>We set <code>precision</code> to <code>float16</code> because quantization is not supported for the <code>bfloat16</code> data type as of this writing.</p>
<p>Repeat the other processes as shown previously.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In case of any questions, submit an <a href="https://github.com/neuralmagic/sparseml" target="_blank" rel="noopener noreferrer">issue on GItHub</a> or join other LLM developers on our <a href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ" target="_blank" rel="noopener noreferrer">community</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/neuralmagic/docs-v2/tree/main/docs/guides/one-shot-llms-with-sparseml.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs-v2/guides/hf-llm-to-deepsparse"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Convert LLMs from HF</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs-v2/guides/llm-serving-on-windows"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">LLM Serving on Windows</div></a></nav></div></div><div class="col col--3"><div class="wrapper_Y1C6"><div class="section_EsUi"><h3 class="sectionTitle_pv0_">Content</h3><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#note-on-system-requirements" class="table-of-contents__link toc-highlight">Note on system requirements</a></li><li><a href="#how-to-clone-and-install-the-latest-sparseml" class="table-of-contents__link toc-highlight">How to Clone and Install the Latest SparseML</a></li><li><a href="#how-to-one-shot-tinyllama" class="table-of-contents__link toc-highlight">How to One-shot TinyLlama</a></li><li><a href="#how-to-evaluate-the-one-shot-model" class="table-of-contents__link toc-highlight">How to Evaluate the One-shot Model</a></li><li><a href="#how-to-export-the-one-shot-model" class="table-of-contents__link toc-highlight">How to Export the One-shot Model</a></li><li><a href="#using-the-model-with-deepsparse" class="table-of-contents__link toc-highlight">Using the Model With DeepSparse</a></li><li><a href="#upload-model-to-hugging-face" class="table-of-contents__link toc-highlight">Upload Model to Hugging Face</a></li><li><a href="#explaining-the-tinyllama-recipe" class="table-of-contents__link toc-highlight">Explaining the TinyLlama Recipe</a></li><li><a href="#how-to-adapt-a-recipe-for-a-new-model" class="table-of-contents__link toc-highlight">How to Adapt a Recipe for a New Model</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div><div class="section_EsUi"><h3 class="sectionTitle_pv0_">Actions</h3><div class="sectionLinks_oh4b"><a href="https://github.com/neuralmagic/docs-v2/tree/main/docs/guides/one-shot-llms-with-sparseml.mdx" target="_blank" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 -960 960 960" class="icon_Oxux" fill="var(--ifm-toc-link-color)"><path d="M200-200h57l391-391-57-57-391 391v57Zm-80 80v-170l528-527q12-11 26.5-17t30.5-6q16 0 31 6t26 18l55 56q12 11 17.5 26t5.5 30q0 16-5.5 30.5T817-647L290-120H120Zm640-584-56-56 56 56Zm-141 85-28-29 57 57-29-28Z"></path></svg> Edit this page</a></div></div><div class="section_EsUi"><h3 class="sectionTitle_pv0_">Support</h3><div class="sectionLinks_oh4b"><li><a href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ" target="_blank" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 127 127" class="icon_Oxux"><path d="M27.2 80c0 7.3-5.9 13.2-13.2 13.2C6.7 93.2.8 87.3.8 80c0-7.3 5.9-13.2 13.2-13.2h13.2V80zm6.6 0c0-7.3 5.9-13.2 13.2-13.2 7.3 0 13.2 5.9 13.2 13.2v33c0 7.3-5.9 13.2-13.2 13.2-7.3 0-13.2-5.9-13.2-13.2V80z" fill="#E01E5A"></path><path d="M47 27c-7.3 0-13.2-5.9-13.2-13.2C33.8 6.5 39.7.6 47 .6c7.3 0 13.2 5.9 13.2 13.2V27H47zm0 6.7c7.3 0 13.2 5.9 13.2 13.2 0 7.3-5.9 13.2-13.2 13.2H13.9C6.6 60.1.7 54.2.7 46.9c0-7.3 5.9-13.2 13.2-13.2H47z" fill="#36C5F0"></path><path d="M99.9 46.9c0-7.3 5.9-13.2 13.2-13.2 7.3 0 13.2 5.9 13.2 13.2 0 7.3-5.9 13.2-13.2 13.2H99.9V46.9zm-6.6 0c0 7.3-5.9 13.2-13.2 13.2-7.3 0-13.2-5.9-13.2-13.2V13.8C66.9 6.5 72.8.6 80.1.6c7.3 0 13.2 5.9 13.2 13.2v33.1z" fill="#2EB67D"></path><path d="M80.1 99.8c7.3 0 13.2 5.9 13.2 13.2 0 7.3-5.9 13.2-13.2 13.2-7.3 0-13.2-5.9-13.2-13.2V99.8h13.2zm0-6.6c-7.3 0-13.2-5.9-13.2-13.2 0-7.3 5.9-13.2 13.2-13.2h33.1c7.3 0 13.2 5.9 13.2 13.2 0 7.3-5.9 13.2-13.2 13.2H80.1z" fill="#ECB22E"></path></svg> Community Slack</a><a href="https://support.neuralmagic.com" target="_blank" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 -960 960 960" class="icon_Oxux" fill="var(--ifm-toc-link-color)"><path d="M440-120v-80h320v-284q0-117-81.5-198.5T480-764q-117 0-198.5 81.5T200-484v244h-40q-33 0-56.5-23.5T80-320v-80q0-21 10.5-39.5T120-469l3-53q8-68 39.5-126t79-101q47.5-43 109-67T480-840q68 0 129 24t109 66.5Q766-707 797-649t40 126l3 52q19 9 29.5 27t10.5 38v92q0 20-10.5 38T840-249v49q0 33-23.5 56.5T760-120H440Zm-80-280q-17 0-28.5-11.5T320-440q0-17 11.5-28.5T360-480q17 0 28.5 11.5T400-440q0 17-11.5 28.5T360-400Zm240 0q-17 0-28.5-11.5T560-440q0-17 11.5-28.5T600-480q17 0 28.5 11.5T640-440q0 17-11.5 28.5T600-400Zm-359-62q-7-106 64-182t177-76q89 0 156.5 56.5T720-519q-91-1-167.5-49T435-698q-16 80-67.5 142.5T241-462Z"></path></svg> Enterprise Support</a></li></div></div><div class="section_EsUi"><h3 class="sectionTitle_pv0_">Issues</h3><div class="sectionLinks_oh4b"><li><a href="https://github.com/neuralmagic/deepsparse/issues" target="_blank" rel="noopener noreferrer" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" fill="none" class="icon_Oxux"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z" transform="scale(64)" fill="#1B1F23"></path></svg>DeepSparse</a></li><li><a href="https://github.com/neuralmagic/sparseml/issues" target="_blank" rel="noopener noreferrer" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" fill="none" class="icon_Oxux"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z" transform="scale(64)" fill="#1B1F23"></path></svg>SparseML</a></li><li><a href="https://github.com/neuralmagic/sparsezoo/issues" target="_blank" rel="noopener noreferrer" class="link_RNAl"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" fill="none" class="icon_Oxux"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z" transform="scale(64)" fill="#1B1F23"></path></svg>SparseZoo</a></li></div></div></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><span class="footer__link-item">
            <a href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ" target="_blank">
              <svg width="26" height="26" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5.54 16.166c0 1.022-.823 1.857-1.83 1.857-1.008 0-1.831-.835-1.831-1.857s.823-1.857 1.83-1.857H5.54v1.857ZM7.223 16.166c0-1.388 1.117-2.521 2.485-2.521 1.368 0 2.485 1.133 2.485 2.521v6.313c0 1.387-1.117 2.521-2.485 2.521-1.368 0-2.485-1.134-2.485-2.521v-6.313h0ZM9.932 5.037c-1.071 0-1.947-.889-1.947-1.976s.876-1.976 1.947-1.976c1.072 0 1.948.889 1.948 1.976v1.976H9.932ZM9.708 6.673c1.368 0 2.486 1.133 2.486 2.52 0 1.389-1.118 2.522-2.486 2.522H3.485C2.117 11.715 1 10.582 1 9.194s1.117-2.521 2.485-2.521h6.223ZM20.45 9.834c0-1.087.876-1.976 1.948-1.976s1.948.889 1.948 1.976-.876 1.976-1.948 1.976h-1.947V9.834Z" fill="#052D52" stroke="#fff" stroke-width="1.25" stroke-miterlimit="10" stroke-linecap="round" stroke-linejoin="round"></path><path d="M18.777 9.834c0 1.388-1.117 2.521-2.485 2.521-1.368 0-2.485-1.133-2.485-2.521V3.521C13.807 2.134 14.924 1 16.292 1c1.368 0 2.485 1.134 2.485 2.521v6.313h0ZM16.292 20.722c1.072 0 1.947.888 1.947 1.975 0 1.088-.875 1.976-1.947 1.976s-1.948-.888-1.948-1.976v-1.975h1.948v0ZM16.292 19.025c-1.368 0-2.485-1.134-2.485-2.522 0-1.387 1.117-2.52 2.485-2.52h6.223c1.368 0 2.485 1.133 2.485 2.52 0 1.388-1.117 2.522-2.485 2.522h-6.223Z" stroke="#fff" stroke-width="1.25" stroke-miterlimit="10" stroke-linecap="round" stroke-linejoin="round"></path></svg>
            </a></span><span class="footer__link-separator">·</span><span class="footer__link-item">
            <a href="https://github.com/neuralmagic" target="_blank">
                <svg width="26" height="26" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M15.5 25a.496.496 0 0 1-.313-.111.509.509 0 0 1-.187-.395V20.73c0-.91-.115-1.555-.363-2.029a.51.51 0 0 1-.013-.446.5.5 0 0 1 .342-.282C17.93 17.281 20 15.086 20 12.636c0-1.214-.493-2.373-1.425-3.351a.51.51 0 0 1-.094-.565c.372-.811.293-1.916-.148-2.579-.583.23-1.34.713-1.832 1.304a.493.493 0 0 1-.554.149 8.576 8.576 0 0 0-5.893 0 .497.497 0 0 1-.554-.148c-.492-.59-1.249-1.073-1.833-1.304-.441.663-.52 1.768-.148 2.58.087.188.05.412-.094.563C6.493 10.263 6 11.423 6 12.635c0 2.335 1.863 4.438 4.636 5.232a.506.506 0 0 1 .364.488v.353c0 .633-.251.99-.462 1.18-.452.401-1.036.34-1.1.333h-.01c-.824 0-1.444-.464-2.043-.913-.301-.226-.606-.456-.961-.646.077.105.153.213.23.322.75 1.055 1.599 2.252 2.847 2.252h1c.276 0 .5.227.5.506v2.752a.509.509 0 0 1-.187.395.505.505 0 0 1-.421.099C4.95 23.766 1 18.785 1 13.142 1 6.447 6.383 1 13 1s12 5.447 12 12.142c0 5.642-3.95 10.624-9.392 11.846A.49.49 0 0 1 15.5 25Zm.258-6.193c.164.523.242 1.15.242 1.923v3.114c4.671-1.341 8-5.744 8-10.702 0-6.137-4.935-11.13-11-11.13S2 7.005 2 13.142c0 4.959 3.329 9.36 8 10.703v-1.597h-.5c-1.76 0-2.813-1.482-3.659-2.674-.479-.675-.975-1.373-1.341-1.373a.503.503 0 0 1-.5-.506c0-.28.224-.506.5-.506 1.74 0 2.705.723 3.48 1.305.536.402.958.718 1.52.718.056.003.263.019.379-.087.095-.087.119-.26.121-.396-3.006-.999-5-3.408-5-6.092 0-1.38.512-2.692 1.484-3.81-.429-1.258-.164-2.794.662-3.631a.49.49 0 0 1 .481-.132c.668.18 1.66.705 2.401 1.469a9.598 9.598 0 0 1 5.941 0c.741-.764 1.733-1.29 2.401-1.469a.489.489 0 0 1 .481.132c.827.837 1.091 2.373.662 3.63.975 1.12 1.487 2.43 1.487 3.81 0 2.76-2.127 5.23-5.242 6.17Z" fill="#fff" stroke="#fff" stroke-width="0.2"></path></svg>
            </a></span><span class="footer__link-separator">·</span><span class="footer__link-item">
            <a href="https://twitter.com/neuralmagic" target="_blank">
                <svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#twitter_svg__a)"><path d="m13.874 10.445 9.164 13.043h-5.925l-6.394-9.101-.365-.52-.42.477-8.043 9.144h-.785l8.64-9.821.26-.296-.227-.322L.962.5h5.925l6.047 8.607.365.52.42-.477L21.326.5h.785l-8.055 9.157h-.737l.554.788Zm-3.847 1.972.856 1.197.094.131 6.466 9.046.15.209h4.481l-.566-.791-7.924-11.085-.949-1.328-6.096-8.528-.15-.21H1.909l.565.792 7.554 10.567Z" fill="#fff" stroke="#fff"></path></g><defs><clipPath id="twitter_svg__a"><path fill="#fff" d="M0 0h24v24H0z"></path></clipPath></defs></svg>
            </a></span><span class="footer__link-separator">·</span><span class="footer__link-item">
            <a href="https://www.linkedin.com/company/neural-magic" target="_blank">
                <svg width="26" height="26" fill="none" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M22.104 14.94v6.336h-4.078V15.38c0-1.543-.637-2.595-2.04-2.595-1.073 0-1.67.71-1.947 1.394-.105.247-.088.59-.088.932v6.166h-4.04s.053-10.444 0-11.394h4.04v1.789c.239-.782 1.53-1.898 3.589-1.898 2.555 0 4.564 1.638 4.564 5.168ZM4.121 21.276h3.595V9.883H4.121v11.393ZM7.944 6.536c0 1-.764 1.794-1.99 1.794H5.93c-1.18 0-1.946-.79-1.946-1.792 0-1.021.788-1.793 1.994-1.793 1.202 0 1.942.77 1.966 1.791Z" stroke="#fff" stroke-width="1.1" stroke-linejoin="round"></path><path clip-rule="evenodd" d="M23.345 25H2.655A1.655 1.655 0 0 1 1 23.345V2.655C1 1.741 1.741 1 2.655 1h20.69C24.259 1 25 1.741 25 2.655v20.69c0 .914-.741 1.655-1.655 1.655Z" stroke="#fff" stroke-width="1.1" stroke-linejoin="round"></path></svg>
            </a></span><span class="footer__link-separator">·</span><span class="footer__link-item">
            <a href="https://www.youtube.com/channel/UCo8dO_WMGYbWCRnj_Dxr4EA" target="_blank">
                <svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="m9.938 16.077 5.749-3.323-5.75-3.323v6.646Zm12.805-8.673c.144.52.243 1.218.31 2.104.077.887.11 1.65.11 2.315l.067.93c0 2.427-.177 4.21-.487 5.351-.277.997-.92 1.64-1.917 1.916-.52.144-1.473.244-2.935.31-1.44.078-2.758.111-3.977.111l-1.76.067c-4.642 0-7.533-.177-8.674-.488-.997-.277-1.64-.92-1.916-1.916-.144-.52-.244-1.218-.31-2.105-.078-.886-.111-1.65-.111-2.315l-.067-.93c0-2.426.177-4.21.488-5.35.276-.997.919-1.64 1.916-1.917.52-.144 1.473-.243 2.935-.31 1.44-.077 2.758-.11 3.977-.11L12.153 5c4.641 0 7.532.177 8.673.487.997.277 1.64.92 1.917 1.917Z" fill="#052D52" stroke="#fff" stroke-width="1.25"></path></svg>
            </a></span></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Neuralmagic, Inc.</div></div></div></footer></div>
</body>
</html>