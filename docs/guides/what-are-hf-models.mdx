---
sidebar_position: 1
title: What are HF models?
---

# What are HF models?
HF models are DeepSparse-ready models that haven't gone through rigorous testing to ensure they meet certain criteria such as the ones in the SparseZoo. The process enables us to produce models faster without having to wait for the entire quality-assurance process. However, you should always perform evaluation for ensure the model's results are not completely off. 

## How to create HF model 
The first process in creating your own one-shot model is to identify a Hugging Face model you'd like to optimize. [TinyLlama Chat v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) is a great choice for demos becasue if its size. 

Ensure you have the lastest version of SparseML. 
```bash
git clone https://github.com/neuralmagic/sparseml
pip install -e "sparseml[transformers]"

```
Download a recipe and one shot the model.The recipe contains the hyperparameters for pruning and quantization. 
```bash
wget https://huggingface.co/nm-testing/TinyLlama-1.1B-Chat-v0.4-pruned50-quant/raw/main/recipe.yaml # download recipe
sparseml.transformers.text_generation.oneshot --model_name TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dataset_name open_platypus --recipe recipe.yaml --output_dir ./obcq_deployment --precision float16
```
Next evaluate the model using `lm-evaluation-harness`. Start by installing it: 
```bash
git clone https://github.com/neuralmagic/lm-evaluation-harness.git
cd lm-evaluation-harness
pip install -e 
```
Evaluate on hellaswag for example:
```
start=`date +%s`
python main.py \
 --model hf-causal-experimental \
 --model_args pretrained=obcq_deployment,trust_remote_code=True \
 --tasks hellaswag \
 --batch_size 64 \
 --no_cache \
 --write_out \
 --output_path "obcq_deployment/hellaswag.json" \
 --device "cuda:0" \
 --num_fewshot 0
 end=`date +%s`
 echo Execution time was `expr $end - $start` seconds. 
```
Once you are certain that the models performs well, you can export it for use in DeepSparse. 
```bash
sparseml.export --task text-generation obcq_deployment/
```
You can find official models that we have optimized for DeepSparse at https://huggingface.co/neuralmagic and experimental ones at https://huggingface.co/nm-testing. 

If you have any queries about the entire process, [send us a message on Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ). 
