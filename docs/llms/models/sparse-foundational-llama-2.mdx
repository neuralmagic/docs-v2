---
tags:
- llama2
- foundational-models
- sparsified-models keywords:
- llama2
- sparse-llama
- efficient-inference
keywords:
- llama2
- foundational-models
- sparsified-models keywords:
- llama2
- sparse-llama
- efficient-inference
description: TODO
sidebar_position: 1
sidebar_label: Llama 2
---

# Sparse Foundational Llama 2 Models

Neural Magic and Cerebras partnered to offer a range of expertly optimized Llama 2-based Large Language Models (LLMs) that have been sparsified for superior performance and reduced footprint.
These models are carefully selected and rigorously tested, ensuring exceptional quality and seamless deployment.

## Why Choose Sparse Llama 2 Models?

- *Accelerated Inference*: Sparse Llama 2 models offer significant speed improvements, enabling faster responses and real-time applications.
- *Reduced Resource Requirements*: Sparsification decreases the model's size, allowing deployment on edge devices or in environments with limited compute power.
- *Cost-Effectiveness*: Lower compute requirements translate to reduced operational costs for your LLM-based applications.

## Demo

<DocCardList>
    <a href="https://huggingface.co/spaces/neuralmagic/llama-2-sparse-transfer-chat-deepsparse">
        <h3>Sparse Llama 2 Chat Demo</h3>
        <span>Experience the capabilities of Sparse Llama 2 models firsthand with our HF Spaces interactive demo highlighting their performance and potential applications.</span>
    </a>
</DocCardList>


## Models

Currently, the following Sparse Llama 2 models are available for immediate use:
- **Sparse Llama2-7B Pretrained**: A versatile and powerful LLM that has been sparsified for finetuning onto your specific use case.
- **Sparse Llama2-7B Chat**: A specialized variant of the Sparse Llama2-7B model, optimized for chatbot applications.
- **Sparse Llama2-7B Code Generation**: A specialized variant of the Sparse Llama2-7B model, optimized for code generation tasks.
- **Sparse Llama2-7B Instruction Tuning**: A specialized variant of the Sparse Llama2-7B model, optimized for instruction tuning tasks.

<DocCardList>
    <a href="https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21">
        <h3>Sparse Llama 2 Models on Hugging Face</h3>
        <span>Explore Neural Magic's collection of Sparse Llama 2 models, including the versatile Sparse Llama2-7B and other specialized variants. Select the model that best aligns with your use case.</span>
    </a>
</DocCardList>

## Deploy

<DocCardList>
    <a href="https://github.com/neuralmagic/deepsparse">
        <h3>DeepSparse</h3>
        <span>Deploy Sparse Llama 2 models with DeepSparse, Neural Magic's platform for efficient and seamless model deployment on CPUs.</span>
    </a>
</DocCardList>

## Optimize

<DocCardList>
    <a href="https://github.com/neuralmagic/sparseml">
        <h3>SparseML</h3>
        <span>Optimize your Sparse Llama 2 models even further using SparseML. Discover pre-configured recipes and customize optimization strategies to maximize performance for your specific use case.</span>
    </a>
    <a href="https://www.cerebras.net/blog/sparsity-made-easy-introducing-the-cerebras-pytorch-sparsity-library">
        <h3>Cerebras Sparse Training</h3>
        <span>Train your own Sparse Llama 2 models using Cerebras' advanced sparse training capabilities, ensuring optimal performance and efficiency.</span>
    </a>
</DocCardList>