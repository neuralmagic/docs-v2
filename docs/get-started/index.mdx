---
tags:
- Neural Magic
- Deep Learning
- AI Optimization
- Sparsity
- Model Deployment
- Machine Learning
- Model Fine-tuning
keywords:
- Neural Magic
- DeepSparse
- SparseML
- SparseZoo
- CPU Performance
- Model Optimization
- Sparsification
- Deep Learning Models
- AI Deployment
description: Begin your journey with Neural Magic by learning how to install DeepSparse, SparseML, and SparseZoo. Discover how to deploy and fine-tune sparsified models for optimized performance on CPU resources.
sidebar_position: 1
---

# Getting Started

Welcome to the beginning of your journey with Neural Magic!
This section is designed to guide you through the initial steps required to leverage the power of Neural Magicâ€™s software for your AI and deep learning applications.

Here, you'll learn how to install our key products, deploy a model, and fine-tune a sparsified model.
Each step is crafted to provide you with a comprehensive understanding and hands-on experience with our tools.

## Installation

The first step in utilizing Neural Magicâ€™s software suite is to install the necessary components.
Whether you are working with DeepSparse for deploying models, SparseML for model optimization, or SparseZoo for accessing a variety of sparsified models, we have you covered.

<DocCardList items={['get-started/install/index']} />

## Deploy a Model

Once your environment is set up, it's time to deploy a model.
This section will guide you through the process of deploying a pre-sparsified, SparseZoo or custom using DeepSparse, ensuring you achieve optimal performance and accuracy.

<DocCardList items={['get-started/deploy']} />

## Fine-tune a Sparsified Model

After deploying a model, you might want to fine-tune it to your own data to better fit your specific use case.
Fine-tuning a sparsified model will transfer the performant sparse architecture onto a new dataset the same as fine-tuning a dense model works.

<DocCardList items={['get-started/finetune']} />

<br></br>

---

ðŸ”— Ready to dive in? Choose a topic above to get started.

ðŸ“š Remember, if you encounter any challenges or have questions, our [community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) is here to help. Also, stay updated by [subscribing](https://neuralmagic.com/deep-sparse-community/#subscribe) to our newsletter.

ðŸŒŸ Your contributions matter! Help us improve by providing feedback or contributing to our [GitHub repositories](https://github.com/neuralmagic).
