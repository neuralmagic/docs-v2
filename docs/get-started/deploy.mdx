---
tags:
- Neural Magic
- DeepSparse
- SparseZoo
- SparseML
- AI Deployment
- Model Optimization
keywords:
- Neural Network Deployment
- Sparsity
- Performance Optimization
- AI Inference
- Model Benchmarking
description: A comprehensive guide for deploying neural networks with Neural Magicâ€™s DeepSparse, SparseZoo, and SparseML, focusing on text generation and object detection.
sidebar_label: Deploy
sidebar_position: 2
---

# Deployment

This guide provides detailed instructions for deploying neural networks using Neural Magic's suite, including DeepSparse, SparseZoo, and SparseML.
You'll learn about the key processes such as benchmarking, pipeline creation, and server setup, focusing on text generation and object detection models.

## Generative AI - LLMs

:::note
Ensure you've installed the required packages and dependencies for Generative AI as outlined in the [Install Guide](./install#generative-ai).
:::

This section covers deploying large language models (LLMs) with DeepSparse including benchmarking, pipeline creation, and server setup utilizing a pre-sparsified model from the SparseZoo.

The examples below use a sparse, quantized Llama 2 7b chat model to demonstrate the deployment process.
The model is referenced by the following SparseZoo stub for use in the Neural Magic suite:
```text
zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized
```

For other models that work with these examples, browse through the [Generative AI models in the SparseZoo](https://sparsezoo.neuralmagic.com/?modelSet=generative_ai) to find one that fits your needs.

<details>
<summary>Benchmarking</summary>

Benchmarking is an optional step that demonstrates how to validate the performance for both dense and sparsified models, while showcasing the benefits of sparsity in the process.

First benchmark an unoptimized version of the model to establish the baseline.
The SparseZoo stub for the dense model is:
```text
zoo:llama2-7b-ultrachat200k_llama2_pretrain-base
```

Utilizing this stub, benchmark the dense, baseline model with the following command:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.benchmark "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import benchmark

result = benchmark("zoo:llama2-7b-ultrachat200k_llama2_pretrain-base")
print(result)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the baseline model achieves a throughput of `x` tokens per second on a 4-core Intel CPU.

Next, benchmark the sparsified model to compare the performance:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.benchmark "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import benchmark

result = benchmark("zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized")
print(result)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

The sparsified model achieves a throughput of `y` tokens per second on the same CPU, which is `z` times faster than the baseline model!

</details>

<details>
<summary>Evaluating</summary>

This is an optional step that demonstrates how to evaluate a model on a given dataset or set of datasets using DeepSparse to validate and compare the accuracies of dense and sparsified models.

First, evaluate the baseline model to establish the accuracy baseline.
The SparseZoo stub for the dense model is:
```text
zoo:llama2-7b-ultrachat200k_llama2_pretrain-base
```

Utilizing this stub, evaluate the dense, baseline model utilizing the `lm-eval-harness` with the following command:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.evaluate "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" --integration lm

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import evaluate

result = evaluate("zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized", integration="lm")
print(result)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the baseline model achieves a perplexity of `x` on the validation dataset.

Next, evaluate the sparsified model to compare the accuracy:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.evaluate "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" --integration lm

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import evaluate

result = evaluate("zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized", integration="lm")
print(result)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

The sparsified model achieves a perplexity of `y` on the validation dataset, which is within `z` of the baseline model!
</details>

### Pipeline

DeepSparse pipelines for LLMs match the Hugging Face Transformers Python API, allowing for familiarity and easy integration with existing codebases.
To create a text generation pipeline for the example model and generate text, utilize the following code:

```python
from deepsparse import TextGeneration

pipeline = TextGeneration("zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized")
result = pipeline("Neural Magic is")
print(result)

# <output>
# TODO: add output
# </output>
```

As seen in the output, the pipeline generates text based on the input prompt.

### Server

The DeepSparse Server wraps a pipeline in a REST API, allowing for easy deployment and inference.
For generative LLMS, the server supports the OpenAI inference standards, allowing for familiarity and easy integration with existing codebases.

To create a DeepSparse TextGeneration server that will run on port 5543 (default) with the OpenAI specs, utilize the following code:

```bash
deepsparse.server "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" --integration openai

# <output>
# TODO: add output
# </output>
```

Now with the server running, you can send an HTTP request that conforms to the OpenAI spec to generate text.
Below are examples for using `curl` and `python` to send a request to the server:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
curl http://localhost:5543/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",
        "messages": "Neural Magic is",
        "stream": true
    }'

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
import openai

openai.api_key = "EMPTY"
openai.api_base = "http://localhost:5543/v1"

stream = False
completion = openai.ChatCompletion.create(
    messages="Neural Magic is",
    stream=stream,
    max_tokens=30,
    model="zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",
)

if stream:
    for c in completion:
        print(c)
else:
    print(completion)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the server generates text based on the input prompt.

To complete the sections for Computer Vision (CV) - Object Detection in the deployment guide, let's provide instructions and examples tailored for object detection models using Neural Magic's suite.

## CV - Object Detection

:::note
Ensure you've installed the necessary packages and dependencies for CV - Object Detection as outlined in the [Install Guide](./install#object-detection).
:::

This section covers deploying object detection models with DeepSparse including benchmarking, pipeline creation, and server setup utilizing a pre-sparsified model from the SparseZoo.

The examples below use a sparse, quantized YOLOv8 L model to demonstrate the deployment process.
The model is referenced by the following SparseZoo stub for use in the Neural Magic suite:
```text
zoo:yolov8-l-coco-pruned85_quantized
```

For other models that work with these examples, browse through the [CV - Object Detection YOLOv8 models in the SparseZoo](https://sparsezoo.neuralmagic.com/?architectures=yolov8&datasets=coco&tasks=detection&modelSet=computer_vision) to find one that fits your needs.

Additionally, the Pipeline and Server examples expect an image to be located at `image.jpg` in the current working directory.
To download an example image, run the following command:
```bash
wget -O image.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg
```

<details>
<summary>Benchmarking</summary>

Benchmarking is an optional step that demonstrates how to validate the performance for both dense and sparsified models, while showcasing the benefits of sparsity in the process.

First benchmark an unoptimized version of the model to establish the baseline.
The SparseZoo stub for the dense model is:
```text
zoo:yolov8-l-coco-base
```

Utilizing this stub, benchmark the dense, baseline model with the following command:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.benchmark "zoo:yolov8-l-coco-base"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import benchmark

result = benchmark("zoo:yolov8-l-coco-base")
print(result)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the baseline model achieves a throughput of `x` images per second on a 4-core Intel CPU.

Next, benchmark the sparsified model to compare the performance:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.benchmark "zoo:yolov8-l-coco-pruned85_quantized"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
from deepsparse import benchmark

result = benchmark("zoo:yolov8-l-coco-pruned85_quantized")
print(result)

# <output>
# TODO: add output
# </output>
```
</TabItem>
</Tabs>

The sparsified model achieves a throughput of `y` images per second on the same CPU, which is `z` times faster than the baseline model!

</details>

<details>
<summary>Evaluating</summary>

This is an optional step that demonstrates how to evaluate a model on a given dataset or set of datasets using DeepSparse to validate and compare the accuracies of dense and sparsified models.

First, evaluate the baseline model to establish the accuracy baseline.
The SparseZoo stub for the dense model is:
```text
zoo:yolov8-l-coco-base
```

Utilizing this stub, evaluate the dense, baseline model on the COCO128 dataset (default) with the following command:
<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.yolov8.eval --model_path "zoo:yolov8-l-coco-base"

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the baseline model achieves a mAP of `x` on the validation dataset.

Next, evaluate the sparsified model to compare the accuracy:

<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
deepsparse.yolov8.eval --model_path "zoo:yolov8-l-coco-pruned85_quantized"

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

The sparsified model achieves a mAP of `y` on the validation dataset, which is within `z` of the baseline model!

</details>

### Pipeline

DeepSparse pipelines for YOLOv8 models accept an image or images as inputs and return a list of bounding boxes and their associated classes and confidences.
To create an object detection pipeline for the example model and process an image, utilize the following code:

```python
from deepsparse import Pipeline

pipeline = Pipeline.create(task="yolov8", model_path="zoo:yolov8-l-coco-pruned85_quantized")
result = pipeline("basilica.jpg")
print(result)

# <output>
# TODO: add output
# </output>
```

This code initializes an object detection pipeline and processes an image.

### Server

The DeepSparse Server wraps a pipeline in a REST API, allowing for easy deployment and inference.
For object detection models, the server supports the MLServer inference standards, allowing for familiarity and easy integration with existing codebases.

To create a DeepSparse Pipeline server that will run on port 5543 (default) with the MLServer specs, utilize the following code:

```bash
deepsparse.server "zoo:yolov8-l-coco-pruned85_quantized"
```

Now with the server running, you can send an HTTP request that conforms to the MLServer spec to process an image.
Below are examples for using `curl` and `python` to send a request to the server:

<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
curl -X POST http://localhost:5543/predict \
    -H "Content-Type: application/json" \
    -d '{"inputs": ["path/to/image.jpg"]}'

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python">

```python
import requests

response = requests.post(
    "http://localhost:5543/predict",
    json={"inputs": ["path/to/image.jpg"]}
)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

As seen in the output, the server processes the image and returns a list of bounding boxes and their associated classes and confidences.


---

For more information on the example tasks in this guide, other supported tasks, and custom integrations dive into the following resources:
- [Generative AI](../generative-ai)
- [Computer Vision](../computer-vision)
- [Natural Language Processing](../nlp)
- [Custom Integrations](../custom-integrations)
