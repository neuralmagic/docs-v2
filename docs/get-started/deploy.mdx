---
tags:
- Neural Magic
- DeepSparse
- SparseZoo
- SparseML
- AI Deployment
- Model Optimization
keywords:
- Neural Network Deployment
- Sparsity
- Performance Optimization
- AI Inference
- Model Benchmarking
description: A comprehensive guide for deploying neural networks with Neural Magicâ€™s DeepSparse, SparseZoo, and SparseML, focusing on text generation and object detection.
sidebar_label: Deploy
sidebar_position: 2
---

# Deployment

This guide provides detailed instructions for deploying neural networks using Neural Magic's suite, including DeepSparse, SparseZoo, and SparseML.
You'll learn about the key processes such as benchmarking, pipeline creation, and server setup, focusing on text generation models.

## Causal Language Modeling - LLMs

:::note
Ensure you've installed the required packages and dependencies for Generative AI as outlined in the [Install Guide](./install#generative-ai).
:::

This section covers deploying large language models (LLMs) with DeepSparse including benchmarking, pipeline creation, and server setup utilizing a pre-sparsified model from the SparseZoo.

The examples below use a sparse, quantized Llama 2 7b chat model to demonstrate the deployment process.
The model is referenced by the following SparseZoo stub for use in the Neural Magic suite:
```text
zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized
```

For other models that work with these examples, browse through the [Generative AI models in the SparseZoo](https://sparsezoo.neuralmagic.com/?modelSet=generative_ai) to find one that fits your needs.

### Pipeline

DeepSparse pipelines for LLMs match the Hugging Face Transformers Python API, allowing for familiarity and easy integration with existing codebases.
To create a text generation pipeline for the example model and generate text, utilize the following code:

```python
from deepsparse import TextGeneration

pipeline = TextGeneration(
    "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized"
)
result = pipeline("Neural Magic is")
print(result)

# <output>
# TODO: add output
# </output>
```

As seen in the output, the pipeline generates text based on the input prompt.

### Server

The DeepSparse Server wraps a pipeline in a REST API, allowing for easy deployment and inference.
For generative LLMS, the server supports the OpenAI inference standards, allowing for familiarity and easy integration with existing codebases.

To create a DeepSparse TextGeneration server that will run on port 5543 (default) with the OpenAI specs, utilize the following code:

```bash
deepsparse.server \
  "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" \
  --integration openai

# <output>
# TODO: add output
# </output>
```

Now with the server running, you can send an HTTP request that conforms to the OpenAI spec to generate text.
Below are examples for using `curl` and `python` to send a request to the server:
<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        import openai

        openai.api_key = "EMPTY"
        openai.api_base = "http://localhost:5543/v1"

        completion = openai.ChatCompletion.create(
            messages="Neural Magic is",
            stream=True,
            max_tokens=30,
            model="zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",
        )
        for token in completion:
            print(token)

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>

    <TabItem value="bash" label="Bash">

        ```bash
        curl http://localhost:5543/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",
            "messages": "Neural Magic is",
            "stream": true
        }'

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

As seen in the output, the server generates text based on the input prompt.

### Performance

Various performance metrics for inference performance and accuracy can be evaluated using DeepSparse.
The following sections demonstrate how to benchmark the model and evaluate its accuracy.
The sections below are optional and can be skipped.

#### Benchmarking

<details>
    <summary>Baseline</summary>

    To compare the performance of the sparsified model, we utilize a baseline, unoptimized version of the model from the SparseZoo.
    The stub for the corresponding model is:
    ```text
    zoo:llama2-7b-ultrachat200k_llama2_pretrain-base
    ```

    The following command utilizes the baseline stub and DeepSparse to establish the unoptimized model's performance:

    <Tabs>
        <TabItem value="python" label="Python" default>

            ```python
            from deepsparse import benchmark

            result = benchmark("zoo:llama2-7b-ultrachat200k_llama2_pretrain-base")
            print(result)

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
        <TabItem value="bash" label="Bash">

            ```bash
            deepsparse.benchmark "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

    As seen in the output, the baseline model achieves a throughput of `x` tokens per second on a 4-core Intel CPU.
</details>

The following command utilizes the sparsified model and DeepSparse to establish the optimized model's performance:
<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        from deepsparse import benchmark

        result = benchmark("zoo:llama2-7b-ultrachat200k_llama2_pretrain-base")
        print(result)

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
    <TabItem value="bash" label="Bash">

        ```bash
        deepsparse.benchmark "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base"

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

As seen in the output, the sparsified model achieves a throughput of `y` tokens per second on the same CPU, which is `z` times faster than the baseline model!

#### Accuracy

<details>
    <summary>Baseline</summary>

    To compare the performance of the sparsified model, we utilize a baseline, unoptimized version of the model from the SparseZoo.
    The stub for the corresponding model is:
    ```text
    zoo:llama2-7b-ultrachat200k_llama2_pretrain-base
    ```

    The following command utilizes the baseline stub and DeepSparse to establish the unoptimized model's accuracy:

    <Tabs>
        <TabItem value="python" label="Python" default>

            ```python
            from deepsparse import evaluate

            result = evaluate(
                "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base",
                integration="lm-eval-harness"
            )
            print(result)

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
        <TabItem value="bash" label="Bash">

            ```bash
            deepsparse.evaluate \
                "zoo:llama2-7b-ultrachat200k_llama2_pretrain-base" \
                --integration "lm-eval-harness"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

    As seen in the output, the baseline model achieves a throughput of `x` tokens per second on a 4-core Intel CPU.
</details>

The following command utilizes the sparsified model and DeepSparse to establish the optimized model's accuracy within the `lm-eval-harness`:
<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        from deepsparse import evaluate

        result = evaluate(
            "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized",
            integration="lm-eval-harness"
        )
        print(result)

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
    <TabItem value="bash" label="Bash">

        ```bash
        deepsparse.evaluate \
            "zoo:llama2-7b-ultrachat200k_llama2_pretrain-pruned50_quantized" \
            --integration "lm-eval-harness"

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

As seen in the output, the sparsified model achieves a score of `y`, which is within `z` of the baseline model!

---

For more information on the example tasks in this guide, other supported tasks, and custom integrations dive into the following resources:
- [Generative AI](../generative-ai)
- [Computer Vision](../computer-vision)
- [Natural Language Processing](../nlp)
- [Custom Integrations](../custom-integrations)
