---
tags:
- Neural Magic
- DeepSparse
- SparseZoo
- SparseML
- Model Finetuning
- AI Training
keywords:
- Neural Network Finetuning
- Sparse Training
- Model Adaptation
- AI Enhancement
- Model Customization
description: This guide provides an overview of applying sparse finetuning techniques to large language models using SparseML, with examples leveraging SparseZoo for model selection and DeepSparse for efficient inference.
sidebar_label: Sparse Finetuning
sidebar_position: 4
---

# Sparse Finetuning

This section provides a technical overview of getting started with fine-tuning pre-sparsified neural networks to improve performance on specific tasks or datasets, specifically LLMs, using Neural Magic's SparseML for model adaptation, SparseZoo for pre-trained models, and DeepSparse for accelerated inference.

## Causual Language Modeling - LLMs

:::note
Ensure you have the necessary packages installed for Generative AI as outlined in the [Install Guide](./install#generative-ai).
:::

Sparse finetuning LLMs involves the use of SparseML to adapt a pre-sparsified model to a specific task or dataset, enhancing its performance for the targeted use case and leveraging DeepSparse for efficient inference on CPUs.

The examples below demostrate the process of sparse finetuning a pre-sparsified Llama 7b model from the SparseZoo:
```text
zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50
```

Utilizing the following recipe available with the model, which maintains the sparsity of the model while adapting it to the targeted task or dataset and then utilizes SparseGPT to quantize the model after finetuning:
```text
recipe_type=transfer_quantize
```

For additional models, explore the [Generative AI models in the SparseZoo](https://sparsezoo.neuralmagic.com/?modelSet=generative_ai).

### Data Preparation

SparseML requires a dataset to be used for both one shot methods and fine-tuning.
The dataset is used as the training set during finetuning.

For the examples below, we use the Helpful instructions dataset from the Hugging Face H4 team, which is available in the Hugging Face dataset hub:
<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        from datasets import load_dataset

        dataset = load_dataset("HuggingFaceH4/helpful_instructions")
        ```

    </TabItem>

    <TabItem value="bash" label="Bash" default>

        ```bash
        --dataset "HuggingFaceH4/helpful_instructions"
        ```

    </TabItem>
</Tabs>

Replace the dataset with your own dataset or use the provided example.
The simplest way is to use either a Hugging Face dataset or a list of examples and a tokenizer as shown below:

```python
from sparseml.transformers import SparseAutoTokenizer

tokenizer = SparseAutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
dataset = [
    {"prompt": "Make a list of 10 ways to help students improve their study skills. Output:", "completion": "1. Make a schedule for studying and stick to it. 2. Study in the same place every time. 3. Set goals for yourself. 4. Take breaks when you need them. 5. Don't cram before an exam. 6. Get enough sleep. 7. Eat healthy food. 8. Exercise regularly. 9. Find a study partner. 10. Reward yourself after completing a task."},
    {"prompt": "Given a list of numbers, find out if they are prime or not. you can answer "prime" or "not prime". Input: List: 1, 4, 6, 8, 9 Output:", "completion": "not prime"},
    {"prompt": "Should i take this class or not? The professor of this class is not good at all. He doesn't teach well and he is always late for class.", "completion": "No"}
]
```

For comprehensive data preparation guidelines including formats like CSV and JSONL, refer to our [detailed datasets guide](../generative-ai/causal-language-modeling/data).

### Training

Training the sparse model further on the targeted dataset utilizing SparseML and Recipes is simple and straightforward.
In doing this, the performant, sparse architecture is adapted to the specific task or dataset, enhancing its performance for the targeted use case.
To start the training process, utilize the following command:

<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        from sparseml.transformers import (
            SparseAutoModelForCausalLM, SparseAutoTokenizer, load_dataset, apply
        )

        model = SparseAutoModelForCausalLM.from_pretrained(
            "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50"
        )
        tokenizer = SparseAutoTokenizer.from_pretrained(
            "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50"
        )
        dataset = load_dataset("HuggingFaceH4/helpful_instructions")

        apply(
            model=model,
            tokenizer=tokenizer,
            data=dataset,
            recipe="zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize",
            recipe_args={"epochs": 1},
        )

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
    <TabItem value="bash" label="Bash">

        ```bash
        sparseml.transformers.text_generation.apply \
            --model "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50" \
            --data "HuggingFaceH4/helpful_instructions" \
            --recipe "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize" \
            --recipe-args '{"epochs": 1}'

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

Once the training process is complete, the model will be saved to the `./output` directory by default.

### Inference

:::note
Be sure to replace the sample models provided below with the model you finetuned.
A sample, sparse-quantized model from the SparseZoo is used by default with the following stub:
```text
zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized
```
:::

<details>
    <summary>Evaluating Accuracy</summary>

    It is important to evaluate the accuracy of the model to ensure it meets the desired performance requirements.
    To do so, we can use the following code to evaluate the model across a number of benchmarks available in the `lm-eval-harness`:
    <Tabs>
        <TabItem value="python" label="Python" default>

            ```python
            from sparseml import evaluate

            evaluate(
                "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",
                integration="lm-eval-harness"
            )

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
        <TabItem value="bash" label="Bash">

            ```bash
            sparseml.evaluate \
                "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \
                --integration "lm-eval-harness"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

</details>

After sparse finetuning the model, it can be used for inference in PyTorch with the following code:
```python
from sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer

model_path = "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized"
model = SparseAutoModelForCausalLM.from_pretrained(model_path)
tokenizer = SparseAutoTokenizer.from_pretrained(model_path)
inputs = tokenizer(["Neural Magic is"], return_tensors="pt")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print(outputs)

# <output>
# TODO
# </output>
```

The above code; however, does not leverage the sparsity within the model for efficient inference.
To do so, we first export the model to ONNX and inject KV caching logic so that it is ready for efficient inference on CPUs with DeepSparse:
<Tabs>
    <TabItem value="python" label="Python" default>

        ```python
        from sparseml import export

        export(
            "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",
            task="text-generation",
            sequence_length=1024,
            output_path="./exported"
        )

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
    <TabItem value="bash" label="Bash">

        ```bash
        sparseml.export \
            "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \
            --task "text-generation" \
            --sequence_length 1024 \
            --output_path "./exported"

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

The exported model located at `./exported` can now be used for efficient inference with DeepSparse.
To do so, sub in the exported model within the [Getting Started - Deploy](../getting-started/deploy) guide for your desired deployment method.

---

For more information on the example tasks in this guide, other supported tasks, and custom integrations, explore the following resources:
- [Generative AI](../generative-ai)
- [Computer Vision](../computer-vision)
- [Natural Language Processing](../nlp)
- [Custom Integrations](../custom-integrations)