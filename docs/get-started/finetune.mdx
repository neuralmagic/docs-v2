---
tags:
- Neural Magic
- DeepSparse
- SparseZoo
- SparseML
- Model Finetuning
- AI Training
keywords:
- Neural Network Finetuning
- Sparse Training
- Model Adaptation
- AI Enhancement
- Model Customization
description: A detailed guide on finetuning sparse models with Neural Magicâ€™s suite, focusing on techniques for both Generative AI and Object Detection models.
sidebar_label: Finetune
sidebar_position: 3
---

# Sparse Finetuning

This guide provides detailed instructions on finetuning sparse models using Neural Magic's suite, including DeepSparse, SparseZoo, and SparseML.
You'll learn about the benefits of sparse finetuning, which combines the efficiency of sparsity with the adaptability of finetuning, enhancing model performance on specific tasks or datasets.

## Generative AI - LLMs

:::note
Ensure you have the necessary packages installed for Generative AI as outlined in the [Install Guide](./install#generative-ai).
:::

In this section, we focus on finetuning large language models (LLMs) for specific tasks or datasets.
The example model used here is a pre-sparsified, 7b parameter version of Llama 2 that was sparsified on the Platypus instruction tuning dataset from the SparseZoo:
```text
zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50
```

After training, the model will additionally be quantized to 8-bit integer weights and activations, resulting in a model that is both sparse and quantized.
A quantized version of the above model is used in the evaluation and inference sections below with the following stub.
Be sure to replace the stub with the path to the model you trained:
```text
zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized
```

For additional models, explore the [Generative AI models in the SparseZoo](https://sparsezoo.neuralmagic.com/?modelSet=generative_ai).

The example dataset used here is the Helpful Instructions dataset from the Hugging Face H4 team.
Be sure to replace the dataset with your desired one in the training section below:
```text
HuggingFaceH4/helpful_instructions
```

<details>
<summary>Data Preparation</summary>

Preparing your dataset to work with the expected format is key to enabling the finetuning pipelines to work.
Examples are provided below for the supported formats include Hugging Face datasets, CSV files, and JSONL files.

#### Hugging Face Datasets:

<Tabs>

<TabItem value="bash" label="Bash" default>

```bash
--data "HuggingFaceH4/helpful_instructions"
```

</TabItem>

<TabItem value="python" label="Python" default>

```python
from datasets import load_dataset

dataset = load_dataset("HuggingFaceH4/helpful_instructions", name="all")
```

</TabItem>

</Tabs>

#### CSV:

To ensure the CSV file is formatted correctly, it should have two columns: `prompt` and `completion`.
The `prompt` column should contain the prompt text, and the `completion` column should contain the expected completion text.
Here's an example of a CSV file for the helpful instructions dataset formatted correctly:

```csv
prompt,completion
"""Make a list of 10 ways to help students improve their study skills.""","""1. Make a schedule for studying and stick to it. 2. Study in the same place every time. 3. Set goals for yourself. 4. Take breaks when you need them. 5. Don't cram before an exam. 6. Get enough sleep. 7. Eat healthy food. 8. Exercise regularly. 9. Find a study partner. 10. Reward yourself after completing a task."""
"""Task: Find out what are the key topics in the document? output \""topic 1\"", \""topic 2\"", ... , \""topic n\"". The United States has withdrawn from the Paris Climate Agreement.""","""Topic 1"""
"""Given a list of numbers, find out if they are prime or not. you can answer \""prime\"" or \""not prime\"". Input: List: 1, 4, 6, 8, 9""","""not prime"""
"""Task: What are some of your favorite websites, and why do you visit them often?""","""- Stack Overflow - I like to learn new things, and this is a great place to find answers to questions."""
"""Should i take this class or not? The professor of this class is not good at all. He doesn't teach well and he is always late for class.""","""No"""
```

#### JSONL Example:

To ensure the JSONL file is formatted correctly, it should contain a JSON object on each line.
Each JSON object should have a `prompt` key and a `completion` key.
The `prompt` key should contain the prompt text, and the `completion` key should contain the expected completion text.
Here's an example of a JSONL file for the helpful instructions dataset formatted correctly:

```json
{"prompt": "\"Make a list of 10 ways to help students improve their study skills.\"", "completion": "\"1. Make a schedule for studying and stick to it. 2. Study in the same place every time. 3. Set goals for yourself. 4. Take breaks when you need them. 5. Don't cram before an exam. 6. Get enough sleep. 7. Eat healthy food. 8. Exercise regularly. 9. Find a study partner. 10. Reward yourself after completing a task.\""}
{"prompt": "\"Task: Find out what are the key topics in the document? output \\\"topic 1\\\", \\\"topic 2\\\", ... , \\\"topic n\\\". The United States has withdrawn from the Paris Climate Agreement.\"", "completion": "\"Topic 1\""}
{"prompt": "\"Given a list of numbers, find out if they are prime or not. you can answer \\\"prime\\\" or \\\"not prime\\\". Input: List: 1, 4, 6, 8, 9\"", "completion": "\"not prime\""}
{"prompt": "\"Task: What are some of your favorite websites, and why do you visit them often?\"", "completion": "\"- Stack Overflow - I like to learn new things, and this is a great place to find answers to questions.\""}
{"prompt": "\"Should i take this class or not? The professor of this class is not good at all. He doesn't teach well and he is always late for class.\"", "completion": "\"No\""}
```

</details>

### Training

Training the sparse model further on the targeted dataset utilizing SparseML and Recipes is simple and straightforward.
In doing this, the performant, sparse architecture is adapted to the specific task or dataset, enhancing its performance for the targeted use case.
To start the training process, utilize the following command:

<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
sparseml.transformers.text_generation.apply \
    --model "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50" \
    --data "HuggingFaceH4/helpful_instructions" \
    --recipe "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize" \
    --epochs 1

# <output>
# TODO: add output
# </output>
```

</TabItem>

<TabItem value="python" label="Python" default>

```python
from sparseml.transformers.text_generation import apply

apply(
    model="zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50",
    data="HuggingFaceH4/helpful_instructions",
    recipe="zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50?recipe_type=transfer_quantize",
    epochs=1,
)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

Once the training process is complete, the model will be saved to the `./output` directory by default.

### Evaluation

Once the model has been fine-tuned, evaulating its performance on the targeted dataset is simple and straightforward.
SparseML is integrated with numerous evaluation libraries, including LM Eval Harness, Alpaca Eval Harness, BigCode, and more.
For the instruction tuned model created above, we'll use the LM Eval Harness to evaluate its performance across a variety of metrics.
To start the evaluation process, utilize the following command and replace the sample model with the `./output` directory from the training process:

<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
sparseml.evaluate \
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \
    --integration "lm-eval-harness"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python" default>

```python
from sparseml import evaluate

evaluate(
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",
    integration="lm-eval-harness"
)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

### Inference

After finetuning and evaluation, the model is ready for inference.
You can load and utilize this model in PyTorch utilizing the following code:

```python
from sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer

model = SparseAutoModelForCausalLM.from_pretrained(
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized"
)
tokenizer = SparseAutoTokenizer.from_pretrained(
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized"
)
inputs = tokenizer(["PRMOPT"], return_tensors="pt")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print(outputs)

# <output>
# TODO: add output
# </output>
```

However, the above code will not utilize the sparsity of the model.
To do this, you'll export the model to an ONNX format which DeepSparse can utilize.
The following code will export to the proper format in addition to injecting KV Cache for improved performance:

<Tabs>
<TabItem value="bash" label="Bash" default>

```bash
sparseml.export \
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized" \
    --task "text-generation" \
    --sequence_length 1024 \
    --output_path "./exported"

# <output>
# TODO: add output
# </output>
```

</TabItem>
<TabItem value="python" label="Python" default>

```python
from sparseml import export

export(
    "zoo:llama2-7b-open_platypus_orca_llama2_pretrain-pruned50_quantized",
    task="text-generation",
    sequence_length=1024,
    output_path="./exported"
)

# <output>
# TODO: add output
# </output>
```

</TabItem>
</Tabs>

Once the model has been exported, you can utilize the same commands as outlined in the [Getting Started - Deploy Guide](./deploy) to run performance inference on the exported sparse-quantized model.

## CV - Object Detection

:::note
Ensure you've installed the necessary packages for Object Detection as outlined in the [Install Guide](./install#object-detection).
:::

In this section, we cover finetuning object detection models. We use a pre-sparsified YOLOv5 model as an example:
```text
zoo:yolov5-s-coco-pruned80_quantized
```
Discover more models at the [SparseZoo's Object Detection section](https://sparsezoo.neuralmagic.com/?tasks=detection&modelSet=computer_vision).

<details>
    <summary>Data Preparation</summary>

    Preparing your dataset effectively is key to successful finetuning. Supported formats include Hugging Face dataset aliases, CSV, and JSONL. Here are examples for each format:

    - **Hugging Face Dataset Alias**:
    ```python
    from datasets import load_dataset
    dataset = load_dataset('coco', '202

    1')
    ```
    - **CSV Format**:
    ```python
    import pandas as pd
    dataset = pd.read_csv('your_dataset.csv')
    ```
    - **JSONL Format**:
    ```python
    import json
    with open('your_dataset.jsonl', 'r') as file:
    dataset = [json.loads(line) for line in file]
    ```

</details>

### Training

Training your model on specific datasets for object detection tasks can greatly improve its accuracy and efficiency:

- **CLI**:
```bash
sparseml.finetune --model "zoo:yolov5-s-coco-pruned80_quantized" --data "path/to/dataset"
  ```
- **Python API**:
```python
from sparseml import finetune
finetune("zoo:yolov5-s-coco-pruned80_quantized", data_path="path/to/dataset")
  ```

### Evaluation

Evaluating your finetuned model is crucial for assessing its performance:

- **CLI**:
```bash
sparseml.evaluate --model "zoo:yolov5-s-coco-pruned80_quantized" --data "path/to/validation_data"
  ```
- **Python API**:
```python
from sparseml import evaluate
evaluate("zoo:yolov5-s-coco-pruned80_quantized", data_path="path/to/validation_data")
  ```

### Exporting

Export your finetuned model for deployment:

- **CLI**:
```bash
sparseml.export --model "zoo:yolov5-s-coco-pruned80_quantized" --output_path "path/to/exported_model"
  ```
- **Python API**:
```python
from sparseml import export
export("zoo:yolov5-s-coco-pruned80_quantized", output_path="path/to/exported_model")
  ```

### Deployment

For instructions on deploying your finetuned model, see our [Deployment Guide](./deploy).

---

For more information on the example tasks in this guide, other supported tasks, and custom integrations, explore the following resources:
- [Generative AI](../generative-ai)
- [Computer Vision](../computer-vision)
- [Natural Language Processing](../nlp)
- [Custom Integrations](../custom-integrations)