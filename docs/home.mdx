---
slug: /
tags:
- neural-magic
- deep-learning
- model-optimization
- sparsification
- cpu-deployment
keywords:
- Neural Magic
- deep learning optimization
- CPU inference
- sparse models
- SparseML
- DeepSparse
sidebar_label: Home
description: Neural Magic empowers you to optimize and deploy deep learning models on CPUs with GPU-class performance. Unlock efficiency, accessibility, and cost savings with our software solutions.
sidebar_position: 0
---

# Neural Magic Documentation

As a leader in model optimization and inference server acceleration, Neural Magic empowers you to deploy deep learning models with freedom and flexibility. Our software solutions make your practice accessible, efficient, and sustainable across commodity CPUs and now GPUs; more documentation to come in early 2024.

:::tip[FEATURED DEVELOPER HIGHLIGHTS]
_Keep tabs on our innovation with resources, examples, news, and more:_
- Blog: [Neural Magic Leaps Into GPU Acceleration]
(https://neuralmagic.com/blog/bringing-the-neural-magic-to-gpus/)
- New community repo for GPU inferencing: [`nm-vllm`](https://github.com/neuralmagic/nm-vllm)
- [Example notebook](https://github.com/neuralmagic/examples/tree/main/notebooks/marlin-nm-vllm): Quantize LLMs to 4-bit weights and deploy them with Marlin, inside of `nm-vllm`!

:::

![Neural Magic Flows](/img/nm-flows.png)
_Using CPUs_

## Selecting a Model

Start by choosing the ideal model for your project:

- <b>Neural Magic's Model Repositories:</b> Discover a vast selection of pre-sparsified models across popular use cases in our [SparseZoo](https://sparsezoo.neuralmagic.com/) and [Hugging Face](https://huggingface.co/neuralmagic) repositories. These models are ready for immediate, high-performance deployment.
- <b>Custom Models:</b> Easily integrate your PyTorch or ONNX models into Neural Magic's workflow, applying cutting-edge optimization and deployment techniques tailored to your specific needs.

## Optimizing a Model

Achieve unmatched model efficiency with Neural Magic's SparseML. This powerful toolkit leverages state-of-the-art research to streamline your optimization process:

- <b>Advanced Sparsification:</b> Employ sophisticated pruning and quantization strategies to shrink model size and boost inference speed.
- <b>User-Friendly Approach:</b>  SparseML is intuitive and accessible, empowering users at all levels to apply advanced optimization techniques effortlessly.


## Deploying a Model

Deploy your optimized model with exceptional performance on CPUs using Neural Magic's DeepSparse:

- <b>CPU-Optimized Performance:</b> DeepSparse harnesses the potential of sparsity to deliver GPU-level performance on commodity CPUs, maximizing hardware utilization.
- <b>Seamless Integration:</b> Deploy easily, as DeepSparse smoothly integrates into your existing applications, minimizing development overhead.

## Next Steps

Ready to revolutionize your deep learning workflow? Dive into our detailed guides and documentation:

### Sections

<DocCardList items={['get-started/index', 'guides/index', 'products/index', 'details/index']} />

### Tasks

<DocCardList items={['llms/index', 'computer-vision/index', 'nlp/index']} />

<br></br>

---

✅ <b>Connect:</b> Join our [Slack community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) for support.

✅ <b>Subscribe</b> Stay informed with our regular [email updates](https://neuralmagic.com/deep-sparse-community/#subscribe).

✅ <b>Contribute:</b> Shape the future of Neural Magic on [GitHub](https://github.com/neuralmagic) (⭐s appreciated!).

✅ <b>Feedback:</b> Help us refine our [documentation](https://github.com/neuralmagic/docs).
